{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#to plot images\n",
    "import matplotlib.image as mpimg \n",
    "#to read images\n",
    "\n",
    "import cv2 \n",
    "#open CV library for Python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import nms\n",
    "import torchvision.transforms as transforms\n",
    "from statistics import stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python==4.4.0.44 in d:\\anaconda3\\envs\\pytorch37\\lib\\site-packages (4.4.0.44)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\david\\appdata\\roaming\\python\\python37\\site-packages (from opencv-python==4.4.0.44) (1.18.5)\n",
      "Requirement already satisfied: opencv-contrib-python==4.4.0.44 in d:\\anaconda3\\envs\\pytorch37\\lib\\site-packages (4.4.0.44)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\david\\appdata\\roaming\\python\\python37\\site-packages (from opencv-contrib-python==4.4.0.44) (1.18.5)\n",
      "Package                Version\n",
      "---------------------- -------------------\n",
      "-pencv-python          4.4.0.46\n",
      "absl-py                0.11.0\n",
      "argon2-cffi            20.1.0\n",
      "astunparse             1.6.3\n",
      "async-generator        1.10\n",
      "attrs                  20.3.0\n",
      "backcall               0.2.0\n",
      "bleach                 3.2.1\n",
      "cachetools             4.1.1\n",
      "certifi                2020.12.5\n",
      "cffi                   1.14.4\n",
      "chardet                3.0.4\n",
      "cloudpickle            1.6.0\n",
      "colorama               0.4.4\n",
      "cycler                 0.10.0\n",
      "cytoolz                0.11.0\n",
      "dask                   2020.12.0\n",
      "decorator              4.4.2\n",
      "defusedxml             0.6.0\n",
      "entrypoints            0.3\n",
      "gast                   0.3.3\n",
      "google-auth            1.23.0\n",
      "google-auth-oauthlib   0.4.2\n",
      "google-pasta           0.2.0\n",
      "grpcio                 1.34.0\n",
      "h5py                   2.10.0\n",
      "idna                   2.10\n",
      "imagecodecs            2020.5.30\n",
      "imageio                2.9.0\n",
      "importlib-metadata     2.0.0\n",
      "imutils                0.5.3\n",
      "ipykernel              5.3.4\n",
      "ipython                7.19.0\n",
      "ipython-genutils       0.2.0\n",
      "jedi                   0.17.2\n",
      "Jinja2                 2.11.2\n",
      "joblib                 0.17.0\n",
      "jsonschema             3.2.0\n",
      "jupyter-client         6.1.7\n",
      "jupyter-core           4.7.0\n",
      "jupyterlab-pygments    0.1.2\n",
      "Keras-Preprocessing    1.1.2\n",
      "kiwisolver             1.3.1\n",
      "Markdown               3.3.3\n",
      "MarkupSafe             1.1.1\n",
      "matplotlib             3.3.3\n",
      "mistune                0.8.4\n",
      "mkl-fft                1.2.0\n",
      "mkl-random             1.1.1\n",
      "mkl-service            2.3.0\n",
      "nbclient               0.5.1\n",
      "nbconvert              6.0.7\n",
      "nbformat               5.0.8\n",
      "nest-asyncio           1.4.3\n",
      "networkx               2.5\n",
      "notebook               6.1.4\n",
      "numpy                  1.18.5\n",
      "oauthlib               3.1.0\n",
      "olefile                0.46\n",
      "opencv-contrib-python  4.4.0.44\n",
      "opencv-python          4.4.0.44\n",
      "opt-einsum             3.3.0\n",
      "packaging              20.7\n",
      "pandas                 1.1.3\n",
      "pandocfilters          1.4.3\n",
      "parso                  0.7.0\n",
      "pickleshare            0.7.5\n",
      "Pillow                 8.0.1\n",
      "pip                    20.3.1\n",
      "prometheus-client      0.9.0\n",
      "prompt-toolkit         3.0.8\n",
      "protobuf               3.14.0\n",
      "pyasn1                 0.4.8\n",
      "pyasn1-modules         0.2.8\n",
      "pycparser              2.20\n",
      "Pygments               2.7.3\n",
      "pyparsing              2.4.7\n",
      "pyreadline             2.1\n",
      "pyrsistent             0.17.3\n",
      "python-dateutil        2.8.1\n",
      "pytz                   2020.4\n",
      "PyWavelets             1.1.1\n",
      "pywin32                227\n",
      "pywinpty               0.5.7\n",
      "PyYAML                 5.3.1\n",
      "pyzmq                  20.0.0\n",
      "requests               2.25.0\n",
      "requests-oauthlib      1.3.0\n",
      "rsa                    4.6\n",
      "scikit-image           0.17.2\n",
      "scikit-learn           0.23.2\n",
      "scipy                  1.5.4\n",
      "Send2Trash             1.5.0\n",
      "setuptools             51.0.0.post20201207\n",
      "six                    1.15.0\n",
      "tensorboard            2.4.0\n",
      "tensorboard-plugin-wit 1.7.0\n",
      "tensorflow             2.3.1\n",
      "tensorflow-estimator   2.3.0\n",
      "termcolor              1.1.0\n",
      "terminado              0.9.1\n",
      "testpath               0.4.4\n",
      "threadpoolctl          2.1.0\n",
      "tifffile               2020.12.4\n",
      "toolz                  0.11.1\n",
      "torch                  1.7.1\n",
      "torchaudio             0.7.2\n",
      "torchvision            0.8.2\n",
      "tornado                6.1\n",
      "traitlets              5.0.5\n",
      "typing-extensions      3.7.4.3\n",
      "urllib3                1.26.2\n",
      "wcwidth                0.2.5\n",
      "webencodings           0.5.1\n",
      "Werkzeug               1.0.1\n",
      "wheel                  0.36.1\n",
      "wincertstore           0.2\n",
      "wrapt                  1.12.1\n",
      "zipp                   3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python==4.4.0.44\n",
    "!pip install opencv-contrib-python==4.4.0.44\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBGRImage(path):\n",
    "    image = BGR(cv.imread(path))\n",
    "    return image\n",
    "\n",
    "def loadGreyImage(path):\n",
    "    image = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
    "    return image\n",
    "\n",
    "def BGR(image):\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "def displayGreyImage(image, imageName):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "\n",
    "def displayGreyWindows(image, imageName):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def displayBGRImage(image, imageName, size):\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(image)\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def displayBGRImageLarge(image, imageName):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    plt.imshow(image)\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def imageSideBySide(images, imageNames,size):\n",
    "    row = np.ceil(len(images)/20)\n",
    "    fig=plt.figure(figsize=(size, size/2))\n",
    "    for i, image in enumerate(images):\n",
    "        fig.add_subplot(row, 20, i+1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(imageNames[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_dataset', 'train_dataset', 'train_labels']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 25, 26\n",
    "size = 14, 12\n",
    "\n",
    "f = h5py.File('MNIST_synthetic.h5', 'r')\n",
    "\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 101,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.squeeze(np.array(f[\"train_dataset\"])).copy()\n",
    "train_labels = np.squeeze(np.array(f[\"train_labels\"])).copy()\n",
    "test_dataset = np.squeeze(np.array(f[\"test_dataset\"])).copy()\n",
    "\n",
    "singulars_digits = []\n",
    "singulars_labels = []\n",
    "\n",
    "doubles_digits = []\n",
    "doubles_labels = []\n",
    "\n",
    "triples_digits = []\n",
    "triples_labels = []\n",
    "\n",
    "quadruples_digits = []\n",
    "quadruples_labels = []\n",
    "\n",
    "quintuples_digits = []\n",
    "quintuples_labels = []\n",
    "\n",
    "\n",
    "for i, labels in enumerate(train_labels):\n",
    "    if labels[1] == 10:\n",
    "        singulars_digits.append(train_dataset[i])\n",
    "        singulars_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[1] != 10 and labels[2] == 10:\n",
    "        doubles_digits.append(train_dataset[i])\n",
    "        doubles_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[2] != 10 and labels[3] == 10:\n",
    "        triples_digits.append(train_dataset[i])\n",
    "        triples_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[3] != 10 and labels[4] == 10:\n",
    "        quadruples_digits.append(train_dataset[i])\n",
    "        quadruples_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[4] != 10:\n",
    "        quintuples_digits.append(train_dataset[i])\n",
    "        quintuples_labels.append(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "singulars_digits = np.array(singulars_digits)     \n",
    "doubles_digits = np.array(doubles_digits)  \n",
    "triples_digits = np.array(triples_digits)    \n",
    "quadruples_digits = np.array(quadruples_digits)    \n",
    "quintuples_digits = np.array(quintuples_digits)    \n",
    "\n",
    "singulars_labels = np.array(singulars_labels).T[0]\n",
    "doubles_labels = np.array(doubles_labels).T[0:2].T\n",
    "triples_label = np.array(triples_labels).T[0:3].T\n",
    "quadruples_label = np.array(quadruples_labels).T[0:4].T\n",
    "quintuples_label = np.array(quintuples_labels).T[0:5].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_singulars_dataset = []\n",
    "final_singulars_labels = list(singulars_labels)\n",
    "for i, digit in enumerate(singulars_digits):\n",
    "    final_singulars_dataset.append(digit[26:38,26:38])\n",
    "    \n",
    "final_doubles_dataset = []\n",
    "final_doubles_labels = []\n",
    "for i, digit in enumerate(doubles_digits):\n",
    "    final_doubles_dataset.append(digit[26:38,20:32])\n",
    "    final_doubles_dataset.append(digit[26:38,32:44])\n",
    "    final_doubles_labels.append(doubles_labels[i][0])\n",
    "    final_doubles_labels.append(doubles_labels[i][1])\n",
    "\n",
    "final_triples_dataset = []\n",
    "final_triples_labels = []\n",
    "for i, digit in enumerate(triples_digits):\n",
    "    final_triples_dataset.append(digit[26:38,14:26])\n",
    "    final_triples_dataset.append(digit[26:38,26:38])\n",
    "    final_triples_dataset.append(digit[26:38,38:50])\n",
    "    final_triples_labels.append(triples_labels[i][0])\n",
    "    final_triples_labels.append(triples_labels[i][1])\n",
    "    final_triples_labels.append(triples_labels[i][2])\n",
    "    \n",
    "final_quadruples_dataset = []\n",
    "final_quadruples_labels = []\n",
    "for i, digit in enumerate(quadruples_digits):\n",
    "    final_quadruples_dataset.append(digit[26:38,8:20])\n",
    "    final_quadruples_dataset.append(digit[26:38,20:32])\n",
    "    final_quadruples_dataset.append(digit[26:38,32:44])\n",
    "    final_quadruples_dataset.append(digit[26:38,44:56])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][0])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][1])    \n",
    "    final_quadruples_labels.append(quadruples_labels[i][2])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][3]) \n",
    "    \n",
    "final_quintuples_dataset = []\n",
    "final_quintuples_labels = []\n",
    "for i, digit in enumerate(quintuples_digits):\n",
    "    final_quintuples_dataset.append(digit[26:38,2:14])\n",
    "    final_quintuples_dataset.append(digit[26:38,14:26])\n",
    "    final_quintuples_dataset.append(digit[26:38,26:38])\n",
    "    final_quintuples_dataset.append(digit[26:38,38:50])\n",
    "    final_quintuples_dataset.append(digit[26:38,50:62])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][0])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][1])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][2])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][3])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = final_singulars_dataset + final_doubles_dataset + final_triples_dataset + final_quadruples_dataset + final_quintuples_dataset\n",
    "merged_labels = final_singulars_labels + final_doubles_labels + final_triples_labels + final_quadruples_labels + final_quintuples_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 0's in train dataset: 16379\n",
      "number of 1's in train dataset: 18819\n",
      "number of 2's in train dataset: 17095\n",
      "number of 3's in train dataset: 17220\n",
      "number of 4's in train dataset: 16393\n",
      "number of 5's in train dataset: 15275\n",
      "number of 6's in train dataset: 16401\n",
      "number of 7's in train dataset: 17611\n",
      "number of 8's in train dataset: 16175\n",
      "number of 9's in train dataset: 16518\n",
      "total number of digits in train dataset: 167886\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for digit in range(10):\n",
    "    digit_count = list(merged_labels).count(digit)\n",
    "    print(f\"number of {digit}'s in train dataset: {digit_count}\")\n",
    "    count += digit_count\n",
    "\n",
    "print(\"total number of digits in train dataset:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(merged_dataset)))\n",
    "digitsValidationSetSize = int(len(merged_labels) - digitsTrainingSetSize)\n",
    "\n",
    "xValidationSet = []\n",
    "yValidationSet = []\n",
    "\n",
    "for index, digit in enumerate(merged_dataset[0:digitsValidationSetSize]):\n",
    "    xValidationSet.append(digit)\n",
    "    yValidationSet.append(merged_labels[index])\n",
    "\n",
    "xTrainingSet = []\n",
    "yTrainingSet = []\n",
    "\n",
    "start = len(xValidationSet)\n",
    "\n",
    "for i, digit in enumerate(merged_dataset[start:]):\n",
    "    xTrainingSet.append(digit)\n",
    "    yTrainingSet.append(merged_labels[i+start])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding all digits in an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_digits(train_dataset, size):\n",
    "    thresh_train_dataset = train_dataset.copy()\n",
    "    thresh_train_dataset[thresh_train_dataset>30] = 255\n",
    "    thresh_train_dataset[thresh_train_dataset!=255] = 0\n",
    "    #displayGreyImage(thresh_train_dataset,\"\")\n",
    "    i,j = np.where(thresh_train_dataset[:,:]!=0)\n",
    "    #print(i)\n",
    "    bottom_bound = np.min(i)\n",
    "    top_bound = np.max(i)\n",
    "    left_bound = np.min(j)\n",
    "    right_bound = np.max(j)\n",
    "    #print(top_bound, bottom_bound)\n",
    "\n",
    "    h = top_bound-bottom_bound\n",
    "    w = right_bound-left_bound\n",
    "\n",
    "    bounded_train_dataset = train_dataset[bottom_bound-1:top_bound+2,left_bound-1:right_bound+2]\n",
    "    thresh = bounded_train_dataset.copy()\n",
    "    thresh[thresh>30] = 255\n",
    "    thresh[thresh!=255] = 0\n",
    "    #displayGreyImage(bounded_train_dataset,\"\")\n",
    "\n",
    "    #ret, thresh = cv2.threshold(bounded_train_dataset, 30, 255, 0)\n",
    "    #seg = np.where(np.any(thresh, axis=0)==0)\n",
    "    seg = np.where(np.any(thresh, axis=0)==0)\n",
    "\n",
    "    #print(seg)\n",
    "    seg_list = np.asarray(seg)\n",
    "    seg_list = seg_list[0]\n",
    "    #print(seg_list)\n",
    "\n",
    "    from statistics import stdev\n",
    "    sd =0 # remove this and its return when done\n",
    "    if len(seg_list)>2:\n",
    "        # create a list of the gaps between the consecutive values\n",
    "        gaps = [y - x for x, y in zip(seg_list[:-1], seg_list[1:])]\n",
    "        # have python calculate the standard deviation for the gaps\n",
    "        sd = stdev(gaps)\n",
    "        #print(sd)\n",
    "\n",
    "        # create a list of lists, put the first value of the source data in the first\n",
    "        lists = [[seg_list[0]]]\n",
    "        for x in seg_list[1:]:\n",
    "            # if the gap from the current item to the previous is more than 1 SD\n",
    "            # Note: the previous item is the last item in the last list\n",
    "            # Note: the '> 1' is the part you'd modify to make it stricter or more relaxed\n",
    "            if sd<1.5 and sd>0:\n",
    "                if (x - lists[-1][-1]) / (sd+1e-18) > 1.5:\n",
    "                    # then start a new list\n",
    "                    lists.append([])\n",
    "              # add the current item to the last list in the list\n",
    "                lists[-1].append(x)\n",
    "            elif sd==0:\n",
    "                if (x - lists[-1][-1])>1:\n",
    "                    lists.append([])\n",
    "                lists[-1].append(x)\n",
    "            else:\n",
    "                if (x - lists[-1][-1]) / (sd+1e-18) > 0.8:\n",
    "                  # then start a new list\n",
    "                  lists.append([])\n",
    "                # add the current item to the last list in the list\n",
    "                lists[-1].append(x)\n",
    "\n",
    "        splits = np.asarray([np.ceil(np.mean(lists[i])) for i in range(len(lists))]).astype(int)\n",
    "\n",
    "    else:\n",
    "        splits = np.asarray(seg_list)\n",
    "      #print(splits)\n",
    "\n",
    "    n_digits = len(splits)-1\n",
    "    #digits = np.zeros(n_digits, )\n",
    "    digits = []\n",
    "    for i in range(n_digits):\n",
    "        temp = bounded_train_dataset[:,splits[i]:splits[i+1]]\n",
    "        # if temp is less than recommeded size first pad on left and then on both sides\n",
    "        temp_padded = temp.copy()\n",
    "        if (temp.shape[0] != size) or  (temp.shape[1] != size):\n",
    "            diff_y = size-temp.shape[0]\n",
    "            split_diff_y = diff_y//2\n",
    "            remainder_diff_y = diff_y%2\n",
    "            diff_x = size-temp.shape[1]\n",
    "            split_diff_x = diff_x//2\n",
    "            remainder_diff_x = diff_x%2\n",
    "            #print(diff_y)\n",
    "            #print(split_diff_y)\n",
    "            temp_padded = np.pad(temp, ((split_diff_y+remainder_diff_y,split_diff_y),(split_diff_x+remainder_diff_x,split_diff_x)))\n",
    "\n",
    "        digits.append(temp_padded)\n",
    "      #displayGreyImage(train_dataset[bottom_bound-1:top_bound+2,left_bound-1:right_bound+2],train_labels[367])\n",
    "    return digits, n_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boxes_hacks(image, dig_centers):\n",
    "    \n",
    "    n_dig = len(dig_centers)\n",
    "    digits = []\n",
    "    if n_dig == 1:\n",
    "        digits.append(image[26:38,26:38])\n",
    "    elif n_dig == 2:\n",
    "        digits.append(image[26:38,20:32])\n",
    "        digits.append(image[26:38,32:44])\n",
    "    elif n_dig == 3:\n",
    "        digits.append(image[26:38,14:26])\n",
    "        digits.append(image[26:38,26:38])\n",
    "        digits.append(image[26:38,38:50])\n",
    "    elif n_dig == 4:\n",
    "        digits.append(image[26:38,8:20])\n",
    "        digits.append(image[26:38,20:32])\n",
    "        digits.append(image[26:38,32:44])\n",
    "        digits.append(image[26:38,44:56])\n",
    "    elif n_dig == 5:\n",
    "        digits.append(image[26:38,2:14])\n",
    "        digits.append(image[26:38,14:26])\n",
    "        digits.append(image[26:38,26:38])\n",
    "        digits.append(image[26:38,38:50])\n",
    "        digits.append(image[26:38,50:62])\n",
    "\n",
    "    return digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rng\n",
    "rng.seed(12345)\n",
    "def thresh_callback(src, val=30):\n",
    "    threshold = val\n",
    "\n",
    "    src_gray = cv2.blur(src, (1,1))\n",
    "\n",
    "    #src_gray = bounding_box(src_gray,30)\n",
    "\n",
    "    canny_output = cv2.Canny(src_gray, threshold, threshold * 2)\n",
    "    \n",
    "    \n",
    "    contours, _ = cv2.findContours(canny_output, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    \n",
    "    contours_poly = [None]*len(contours)\n",
    "    boundRect = [None]*len(contours)\n",
    "    centers = [None]*len(contours)\n",
    "    radius = [None]*len(contours)\n",
    "    for i, c in enumerate(contours):\n",
    "        contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "        boundRect[i] = cv2.boundingRect(contours_poly[i])\n",
    "        centers[i], radius[i] = cv2.minEnclosingCircle(contours_poly[i])\n",
    "    \n",
    "\n",
    "    arr_centers = np.asarray(centers)\n",
    "    arr_radius  = np.asarray(radius)\n",
    "\n",
    "    centers_inds = np.argsort(arr_centers[:,0])\n",
    "    sorted_centers = arr_centers[centers_inds[::]]\n",
    "    sorted_radius = arr_radius[centers_inds[::]]\n",
    "    \"\"\"\n",
    "    print(\"before:\", arr_centers)\n",
    "    print(arr_radius)\n",
    "    print(\"after:\", sorted_centers)\n",
    "    print(sorted_radius)\n",
    "    \"\"\"\n",
    "\n",
    "    new_centers = [sorted_centers[i] for i in range(len(sorted_centers)) if sorted_radius[i]>1 and sorted_radius[i]<7]\n",
    "    new_radius  = [sorted_radius[i] for i in range(len(sorted_radius)) if sorted_radius[i]>1 and sorted_radius[i]<7] \n",
    "    \n",
    "    \"\"\"\n",
    "    for i, x in enumerate(new_centers):\n",
    "      print(\"new centers\", x,\"new radius\", new_radius[i])\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    print(boundRect)\n",
    "    \"\"\"\n",
    "\n",
    "    drawing = np.zeros((canny_output.shape[0], canny_output.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(len(centers)):\n",
    "      color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "      cv2.circle(drawing, (int(centers[i][0]), int(centers[i][1])), int(radius[i]), color, 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(new_centers)):\n",
    "        color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "        cv2.circle(drawing, (int(new_centers[i][0]), int(new_centers[i][1])), int(new_radius[i]), color, 2)\n",
    "    \n",
    "    \"\"\"\n",
    "    displayGreyImage(drawing, \"contours\")\n",
    "    \"\"\"\n",
    "    \n",
    "    boxes = []\n",
    "    scores = []\n",
    "    boxes_tensor = torch.empty(size=(len(new_centers), 4))\n",
    "    scores_tensor = torch.empty(len(new_centers))\n",
    "    for i, x in enumerate(new_centers):\n",
    "        #print(int(x[0]-new_radius[i]))\n",
    "        bottom = np.floor(x[1]-new_radius[i]).astype(int)\n",
    "        top = np.ceil(x[1]+new_radius[i]).astype(int)\n",
    "        left = np.floor(x[0]-new_radius[i]).astype(int)\n",
    "        right = np.ceil(x[0]+new_radius[i]).astype(int)\n",
    "        \n",
    "        \"\"\"\n",
    "        print(f'bottom: {bottom} top: {top} left: {left} right: {right}')\n",
    "        \"\"\"\n",
    "\n",
    "        box = src_gray[bottom:top, left:right]\n",
    "        score = box.sum()\n",
    "        scores.append(score)\n",
    "\n",
    "        #displayGreyImage(box, i)\n",
    "        boxes.append(box)\n",
    "        \"\"\"\n",
    "        displayGreyImage(boxes[i], i)\n",
    "        \"\"\"\n",
    "        #box_tensor = torch.tensor([bottom, left, top, right])\n",
    "        #boxes_tensor = torch.cat([box_tensor],0)\n",
    "        boxes_tensor[i] = torch.tensor([left, bottom, right, top])\n",
    "        scores_tensor[i] = torch.tensor([score], dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"boxes:{boxes_tensor} scores: {scores_tensor}\")\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    max_boxes_ind = nms(boxes = boxes_tensor, scores = scores_tensor, iou_threshold=0.1)\n",
    "    \n",
    "    \"\"\"\n",
    "    print(max_boxes_ind)\n",
    "    \"\"\"\n",
    "    \n",
    "    #cv2_imshow(drawing)\n",
    "    \n",
    "    j_list = np.asarray(new_centers)\n",
    "    \"\"\"\n",
    "    print(j_list[:,0])\n",
    "    \"\"\"\n",
    "    j_list = np.asarray(j_list[:,0])\n",
    "    j_list = np.sort(j_list)\n",
    "    \"\"\"\n",
    "    print(\"jlist\", j_list)\n",
    "    #print(j_list[1]-j_list[0])\n",
    "    \"\"\"\n",
    "\n",
    "    # create a list of the diffirence in centers between consecutive values\n",
    "    gaps = [y - x for x, y in zip(j_list[:-1], j_list[1:])]\n",
    "    \"\"\"\n",
    "    print(gaps)\n",
    "    \"\"\"\n",
    "    \n",
    "    # have python calculate the standard deviation for the gaps\n",
    "    #sd = stdev(gaps)\n",
    "    \"\"\"\n",
    "    print(sd)\n",
    "    \"\"\"\n",
    "    # create a list of lists, put the first value of the source data in the first\n",
    "    lists = [[j_list[0]]]\n",
    "    \"\"\"\n",
    "    print(lists)\n",
    "    \"\"\"\n",
    "    radius_lists = [[new_radius[0]]]\n",
    "    mean_radius = np.median(new_radius)\n",
    "    #print(mean_radius)\n",
    "    for i, x in enumerate(j_list[1:]):\n",
    "        \"\"\"\n",
    "        print(x- lists[-1][-1])\n",
    "        print(new_radius[i])\n",
    "        \n",
    "        print(new_radius[i+1])\n",
    "        print(x-new_radius[i] - (lists[-1][-1]+new_radius[i-1]))\n",
    "        \"\"\"\n",
    "        # if the gap from the current item to the previous is more than 1 SD\n",
    "        # Note: the previous item is the last item in the last list\n",
    "        # Note: the '> 1' is the part you'd modify to make it stricter or more relaxed\n",
    "        if np.abs(x - (lists[-1][-1])) > 6: #radius[i] (new_radius[i]+new_radius[i+1])*2/3\n",
    "            lists.append([])\n",
    "            radius_lists.append([])\n",
    "        lists[-1].append(x)\n",
    "        radius_lists[-1].append(new_radius[i])\n",
    "    \"\"\"\n",
    "    print(lists)\n",
    "    print(radius_lists)\n",
    "    \"\"\"\n",
    "    digit_centers = np.asarray([np.mean(lists[i]) for i in range(len(lists))]).astype(int)\n",
    "    digit_radius = np.asarray([np.max(radius_lists[i]) for i in range(len(radius_lists))]).astype(int)\n",
    "\n",
    "    final_boxes = create_boxes_hacks(src, digit_centers)\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(len(final_boxes)):\n",
    "        displayGreyImage(final_boxes[i],\"\")\n",
    "    \"\"\"\n",
    "    \n",
    "    n_digits = len(final_boxes)\n",
    "\n",
    "    \"\"\"\n",
    "    print(digit_centers, digit_radius)\n",
    "    print(digit_centers)\n",
    "    print(len(digit_centers))\n",
    "    \"\"\"\n",
    "    return final_boxes, n_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect at index: 2903\n",
      "incorrect at index: 4555\n",
      "incorrect at index: 7230\n",
      "incorrect at index: 7472\n",
      "incorrect at index: 12016\n",
      "incorrect at index: 12127\n",
      "incorrect at index: 13485\n",
      "incorrect at index: 16017\n",
      "incorrect at index: 18127\n",
      "incorrect at index: 19418\n",
      "incorrect at index: 19580\n",
      "incorrect at index: 19752\n",
      "incorrect at index: 20317\n",
      "incorrect at index: 20829\n",
      "incorrect at index: 24262\n",
      "incorrect at index: 25284\n",
      "incorrect at index: 28261\n",
      "incorrect at index: 32014\n",
      "incorrect at index: 32944\n",
      "incorrect at index: 32954\n",
      "incorrect at index: 33012\n",
      "incorrect at index: 38206\n",
      "incorrect at index: 48432\n",
      "incorrect at index: 50390\n",
      "incorrect at index: 51039\n",
      "total percentage incorrect: 0.044642857142857144 %\n"
     ]
    }
   ],
   "source": [
    "wrong = 0 \n",
    "wrong_arr = []\n",
    "wrong_arr_index = []\n",
    "correct = []\n",
    "\n",
    "for i, sample in enumerate(train_dataset):\n",
    "    snips, n_dig = thresh_callback(sample)\n",
    "    real_num_digits = 5 - list(train_labels[i]).count(10)\n",
    "\n",
    "    if(n_dig != real_num_digits):\n",
    "        print(\"incorrect at index:\", i)\n",
    "        wrong += 1\n",
    "        wrong_arr.append(snips)\n",
    "        wrong_arr_index.append(i)  \n",
    "\n",
    "print(\"total percentage incorrect:\", wrong / len(train_dataset) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "num_test_digits = 0\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    images, n_digits = thresh_callback(sample)\n",
    "   \n",
    "    num_test_digits += n_digits\n",
    "\n",
    "divs = []\n",
    "\n",
    "for div in range(1, 100):\n",
    "    if(num_test_digits % (div) == 0):\n",
    "        divs.append(div)\n",
    "\n",
    "batch_size = divs[-1]\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(input_array, label_array, batch_size):\n",
    "    batched = []\n",
    "    label_batched = []\n",
    "    \n",
    "    for i in range(np.floor(len(input_array)/batch_size).astype(int)):\n",
    "        batched.append(np.expand_dims((np.array(input_array[i*batch_size:i*batch_size+batch_size])).astype(np.single),axis=1))\n",
    "        label_batched.append(label_array[i*batch_size:i*batch_size+batch_size])\n",
    "        \n",
    "    return np.array(batched), label_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, h_layers):\n",
    "        self.num_layers = len(h_layers)\n",
    "        self.h_layers = h_layers      \n",
    "        self.convs = []\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(3 * 3 * 256, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN for hyper-parameter testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_dataset_batch, training_labels_batch = create_batches(xTrainingSet, yTrainingSet, batch_size)\n",
    "validation_dataset_batch, validation_labels_batch = create_batches(xValidationSet, yValidationSet, batch_size)\n",
    "\n",
    "# model hyper-parameters\n",
    "h_layers = ([batch_size, 64])\n",
    "max_iters = 25\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "\n",
    "dont_run = True\n",
    "# train neural network\n",
    "\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "if not dont_run:\n",
    "    net = Net(h_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    for epoch in range(max_iters):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(training_dataset_batch):\n",
    "            inputs = torch.from_numpy(data)\n",
    "            labels = training_labels_batch[i] \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, torch.from_numpy(np.array(labels).astype(np.longlong)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 20 == 19:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    # test validation set on model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(training_dataset_batch):\n",
    "            images = torch.from_numpy(data)\n",
    "            labels = training_labels_batch[i]\n",
    "            labels = torch.from_numpy(np.array(labels).astype(np.longlong))\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if (int(predicted[0]) != int(labels[0])):\n",
    "                None\n",
    "\n",
    "        training_accuracies.append(100 * correct / total)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validation_dataset_batch):\n",
    "            images = torch.from_numpy(data)\n",
    "            labels = validation_labels_batch[i]\n",
    "            labels = torch.from_numpy(np.array(labels).astype(np.longlong))\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if (int(predicted[0]) != int(labels[0])):\n",
    "                None\n",
    "\n",
    "        validation_accuracies.append(100 * correct / total)\n",
    "\n",
    "    #print(f\"validation set accuracy ({total} samples): {(100 * correct / total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-12a87e232a1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_accuracies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_axis\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_axis\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'solid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0my_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_accuracies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch37\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   2893\u001b[0m         \u001b[0mverts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0medgecolors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2894\u001b[0m         \u001b[0mplotnonfinite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplotnonfinite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2896\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2897\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch37\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1438\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch37\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m                          \u001b[1;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 **kwargs)\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch37\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4439\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4441\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y must be the same size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(10, 7))\n",
    "fig.autofmt_xdate()\n",
    "legend = np.zeros(6,dtype=object)\n",
    "#for i in range(2):\n",
    "x_axis = np.arange(0,30)\n",
    "y_axis = training_accuracies\n",
    "plt.scatter(x_axis+1, y_axis)\n",
    "plt.plot(x_axis+1, y_axis, linestyle='solid')\n",
    "y_axis = validation_accuracies\n",
    "plt.scatter(x_axis+1, y_axis)\n",
    "plt.plot(x_axis+1, y_axis, linestyle='solid')\n",
    "\n",
    "legend = (\"Training\",\"Validation\")\n",
    "ax.legend(legend)\n",
    "plt.xlabel('Epoch',fontsize=15)\n",
    "plt.ylabel('Accuracy',fontsize=15)\n",
    "plt.title(\"MNIST dataset prediction using multilayer CNN\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN to predict unseen dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 96,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 2.524\n",
      "[1,    40] loss: 2.099\n",
      "[1,    60] loss: 1.673\n",
      "[1,    80] loss: 1.535\n",
      "[1,   100] loss: 1.260\n",
      "[1,   120] loss: 1.140\n",
      "[1,   140] loss: 1.054\n",
      "[1,   160] loss: 0.770\n",
      "[1,   180] loss: 0.944\n",
      "[1,   200] loss: 0.695\n",
      "[1,   220] loss: 0.764\n",
      "[1,   240] loss: 0.682\n",
      "[1,   260] loss: 0.782\n",
      "[1,   280] loss: 1.090\n",
      "[1,   300] loss: 0.876\n",
      "[1,   320] loss: 0.597\n",
      "[1,   340] loss: 0.694\n",
      "[1,   360] loss: 0.534\n",
      "[1,   380] loss: 0.720\n",
      "[1,   400] loss: 0.806\n",
      "[1,   420] loss: 0.629\n",
      "[1,   440] loss: 0.674\n",
      "[1,   460] loss: 0.634\n",
      "[1,   480] loss: 0.611\n",
      "[1,   500] loss: 0.894\n",
      "[1,   520] loss: 0.902\n",
      "[1,   540] loss: 0.635\n",
      "[1,   560] loss: 0.683\n",
      "[1,   580] loss: 0.652\n",
      "[1,   600] loss: 0.624\n",
      "[1,   620] loss: 0.738\n",
      "[1,   640] loss: 0.590\n",
      "[1,   660] loss: 0.576\n",
      "[1,   680] loss: 0.621\n",
      "[1,   700] loss: 0.717\n",
      "[1,   720] loss: 0.535\n",
      "[1,   740] loss: 0.438\n",
      "[1,   760] loss: 0.473\n",
      "[1,   780] loss: 0.404\n",
      "[1,   800] loss: 0.470\n",
      "[1,   820] loss: 0.766\n",
      "[1,   840] loss: 0.573\n",
      "[1,   860] loss: 0.611\n",
      "[1,   880] loss: 0.447\n",
      "[1,   900] loss: 0.658\n",
      "[1,   920] loss: 0.367\n",
      "[1,   940] loss: 0.966\n",
      "[1,   960] loss: 1.008\n",
      "[1,   980] loss: 0.698\n",
      "[1,  1000] loss: 0.644\n",
      "[1,  1020] loss: 0.587\n",
      "[1,  1040] loss: 0.827\n",
      "[1,  1060] loss: 0.669\n",
      "[1,  1080] loss: 0.880\n",
      "[1,  1100] loss: 1.116\n",
      "[1,  1120] loss: 0.964\n",
      "[1,  1140] loss: 1.018\n",
      "[1,  1160] loss: 0.892\n",
      "[1,  1180] loss: 0.798\n",
      "[1,  1200] loss: 0.817\n",
      "[1,  1220] loss: 0.515\n",
      "[1,  1240] loss: 0.831\n",
      "[1,  1260] loss: 0.623\n",
      "[1,  1280] loss: 1.057\n",
      "[1,  1300] loss: 1.017\n",
      "[1,  1320] loss: 0.804\n",
      "[1,  1340] loss: 0.655\n",
      "[1,  1360] loss: 1.265\n",
      "[1,  1380] loss: 1.066\n",
      "[1,  1400] loss: 0.717\n",
      "[1,  1420] loss: 0.873\n",
      "[1,  1440] loss: 0.629\n",
      "[1,  1460] loss: 0.576\n",
      "[1,  1480] loss: 0.627\n",
      "[1,  1500] loss: 0.839\n",
      "[1,  1520] loss: 0.824\n",
      "[1,  1540] loss: 0.542\n",
      "[1,  1560] loss: 0.648\n",
      "[1,  1580] loss: 1.229\n",
      "[1,  1600] loss: 1.231\n",
      "[1,  1620] loss: 0.806\n",
      "[1,  1640] loss: 0.739\n",
      "[1,  1660] loss: 1.105\n",
      "[1,  1680] loss: 0.768\n",
      "[1,  1700] loss: 0.763\n",
      "[1,  1720] loss: 0.729\n",
      "[1,  1740] loss: 0.725\n",
      "[1,  1760] loss: 0.739\n",
      "[1,  1780] loss: 0.952\n",
      "[1,  1800] loss: 2.244\n",
      "[1,  1820] loss: 2.805\n",
      "[1,  1840] loss: 2.012\n",
      "[1,  1860] loss: 1.352\n",
      "[1,  1880] loss: 1.203\n",
      "[1,  1900] loss: 1.327\n",
      "[1,  1920] loss: 1.415\n",
      "[1,  1940] loss: 1.217\n",
      "[1,  1960] loss: 1.004\n",
      "[1,  1980] loss: 1.012\n",
      "[1,  2000] loss: 0.898\n",
      "[1,  2020] loss: 0.841\n",
      "[1,  2040] loss: 0.908\n",
      "[1,  2060] loss: 0.877\n",
      "[1,  2080] loss: 0.897\n",
      "[1,  2100] loss: 0.814\n",
      "[1,  2120] loss: 0.882\n",
      "[1,  2140] loss: 0.738\n",
      "[1,  2160] loss: 0.716\n",
      "[1,  2180] loss: 0.673\n",
      "[1,  2200] loss: 0.715\n",
      "[1,  2220] loss: 0.686\n",
      "[1,  2240] loss: 0.833\n",
      "[1,  2260] loss: 0.720\n",
      "[1,  2280] loss: 0.771\n",
      "[1,  2300] loss: 0.793\n",
      "[1,  2320] loss: 0.781\n",
      "[1,  2340] loss: 0.654\n",
      "[1,  2360] loss: 0.564\n",
      "[1,  2380] loss: 0.771\n",
      "[1,  2400] loss: 0.890\n",
      "[1,  2420] loss: 0.704\n",
      "[1,  2440] loss: 0.913\n",
      "[1,  2460] loss: 0.741\n",
      "[1,  2480] loss: 0.889\n",
      "[1,  2500] loss: 0.701\n",
      "[1,  2520] loss: 0.758\n",
      "[1,  2540] loss: 0.583\n",
      "[1,  2560] loss: 0.791\n",
      "[1,  2580] loss: 0.815\n",
      "[1,  2600] loss: 0.673\n",
      "[1,  2620] loss: 0.820\n",
      "[1,  2640] loss: 0.710\n",
      "[1,  2660] loss: 0.779\n",
      "[1,  2680] loss: 0.931\n",
      "[1,  2700] loss: 0.825\n",
      "[1,  2720] loss: 0.685\n",
      "[1,  2740] loss: 0.703\n",
      "[1,  2760] loss: 0.885\n",
      "[1,  2780] loss: 0.784\n",
      "[1,  2800] loss: 0.643\n",
      "[1,  2820] loss: 0.885\n",
      "[1,  2840] loss: 0.683\n",
      "[1,  2860] loss: 0.995\n",
      "[1,  2880] loss: 1.083\n",
      "[1,  2900] loss: 0.862\n",
      "[1,  2920] loss: 0.947\n",
      "[1,  2940] loss: 0.868\n",
      "[1,  2960] loss: 0.663\n",
      "[1,  2980] loss: 0.688\n",
      "[1,  3000] loss: 0.571\n",
      "[1,  3020] loss: 0.849\n",
      "[1,  3040] loss: 0.956\n",
      "[1,  3060] loss: 0.741\n",
      "[1,  3080] loss: 1.015\n",
      "[1,  3100] loss: 1.402\n",
      "[1,  3120] loss: 1.221\n",
      "[1,  3140] loss: 0.904\n",
      "[1,  3160] loss: 0.926\n",
      "[1,  3180] loss: 0.781\n",
      "[1,  3200] loss: 0.650\n",
      "[1,  3220] loss: 0.795\n",
      "[1,  3240] loss: 0.820\n",
      "[1,  3260] loss: 0.729\n",
      "[1,  3280] loss: 0.526\n",
      "[1,  3300] loss: 1.062\n",
      "[1,  3320] loss: 0.995\n",
      "[1,  3340] loss: 0.632\n",
      "[1,  3360] loss: 0.683\n",
      "[1,  3380] loss: 0.962\n",
      "[1,  3400] loss: 0.814\n",
      "[1,  3420] loss: 0.885\n",
      "[1,  3440] loss: 0.820\n",
      "[1,  3460] loss: 0.824\n",
      "[1,  3480] loss: 0.660\n",
      "[1,  3500] loss: 0.686\n",
      "[1,  3520] loss: 0.630\n",
      "[1,  3540] loss: 0.808\n",
      "[1,  3560] loss: 0.732\n",
      "[1,  3580] loss: 0.641\n",
      "[1,  3600] loss: 0.665\n",
      "[1,  3620] loss: 0.466\n",
      "[1,  3640] loss: 0.638\n",
      "[1,  3660] loss: 0.947\n",
      "[1,  3680] loss: 0.714\n",
      "[1,  3700] loss: 0.779\n",
      "[1,  3720] loss: 0.920\n",
      "[1,  3740] loss: 0.647\n",
      "[1,  3760] loss: 0.657\n",
      "[1,  3780] loss: 0.684\n",
      "[1,  3800] loss: 0.893\n",
      "[1,  3820] loss: 0.685\n",
      "[1,  3840] loss: 0.631\n",
      "[1,  3860] loss: 0.940\n",
      "[1,  3880] loss: 0.797\n",
      "[1,  3900] loss: 0.884\n",
      "[1,  3920] loss: 0.807\n",
      "[1,  3940] loss: 0.827\n",
      "[1,  3960] loss: 0.707\n",
      "[1,  3980] loss: 1.174\n",
      "[1,  4000] loss: 1.291\n",
      "[1,  4020] loss: 1.040\n",
      "[1,  4040] loss: 1.004\n",
      "[1,  4060] loss: 0.943\n",
      "[1,  4080] loss: 1.007\n",
      "[1,  4100] loss: 1.302\n",
      "[1,  4120] loss: 1.193\n",
      "[1,  4140] loss: 0.958\n",
      "[1,  4160] loss: 1.031\n",
      "[1,  4180] loss: 1.273\n",
      "[1,  4200] loss: 0.922\n",
      "[1,  4220] loss: 1.139\n",
      "[1,  4240] loss: 0.926\n",
      "[1,  4260] loss: 0.897\n",
      "[1,  4280] loss: 0.666\n",
      "[1,  4300] loss: 0.910\n",
      "[1,  4320] loss: 1.088\n",
      "[1,  4340] loss: 1.598\n",
      "[1,  4360] loss: 1.599\n",
      "[1,  4380] loss: 1.413\n",
      "[1,  4400] loss: 1.862\n",
      "[1,  4420] loss: 1.534\n",
      "[1,  4440] loss: 1.467\n",
      "[1,  4460] loss: 1.688\n",
      "[1,  4480] loss: 1.400\n",
      "[1,  4500] loss: 1.196\n",
      "[1,  4520] loss: 1.528\n",
      "[1,  4540] loss: 1.664\n",
      "[1,  4560] loss: 1.204\n",
      "[1,  4580] loss: 0.895\n",
      "[1,  4600] loss: 1.365\n",
      "[1,  4620] loss: 1.204\n",
      "[1,  4640] loss: 1.230\n",
      "[1,  4660] loss: 1.175\n",
      "[1,  4680] loss: 1.210\n",
      "[1,  4700] loss: 1.105\n",
      "[1,  4720] loss: 1.165\n",
      "[1,  4740] loss: 0.945\n",
      "[1,  4760] loss: 1.155\n",
      "[1,  4780] loss: 1.611\n",
      "[1,  4800] loss: 1.421\n",
      "[1,  4820] loss: 1.109\n",
      "[1,  4840] loss: 1.324\n",
      "[1,  4860] loss: 1.376\n",
      "[1,  4880] loss: 1.277\n",
      "[1,  4900] loss: 0.923\n",
      "[1,  4920] loss: 1.094\n",
      "[1,  4940] loss: 1.383\n",
      "[1,  4960] loss: 1.399\n",
      "[1,  4980] loss: 1.210\n",
      "[1,  5000] loss: 1.252\n",
      "[1,  5020] loss: 1.247\n",
      "[1,  5040] loss: 1.118\n",
      "[1,  5060] loss: 0.829\n",
      "[1,  5080] loss: 1.012\n",
      "[1,  5100] loss: 0.880\n",
      "[1,  5120] loss: 0.905\n",
      "[1,  5140] loss: 0.928\n",
      "[1,  5160] loss: 0.910\n",
      "[1,  5180] loss: 0.899\n",
      "[1,  5200] loss: 0.998\n",
      "[1,  5220] loss: 0.884\n",
      "[1,  5240] loss: 1.159\n",
      "[1,  5260] loss: 0.883\n",
      "[1,  5280] loss: 0.838\n",
      "[1,  5300] loss: 0.867\n",
      "[1,  5320] loss: 0.925\n",
      "[1,  5340] loss: 0.969\n",
      "[1,  5360] loss: 0.696\n",
      "[1,  5380] loss: 0.910\n",
      "[1,  5400] loss: 0.960\n",
      "[1,  5420] loss: 0.769\n",
      "[1,  5440] loss: 0.863\n",
      "[1,  5460] loss: 0.869\n",
      "[1,  5480] loss: 0.936\n",
      "[1,  5500] loss: 0.703\n",
      "[1,  5520] loss: 0.710\n",
      "[1,  5540] loss: 1.039\n",
      "[1,  5560] loss: 0.850\n",
      "[1,  5580] loss: 0.890\n",
      "[1,  5600] loss: 0.859\n",
      "[1,  5620] loss: 0.866\n",
      "[1,  5640] loss: 0.887\n",
      "[1,  5660] loss: 0.721\n",
      "[1,  5680] loss: 0.674\n",
      "[1,  5700] loss: 0.903\n",
      "[1,  5720] loss: 0.843\n",
      "[1,  5740] loss: 1.199\n",
      "[1,  5760] loss: 1.187\n",
      "[1,  5780] loss: 0.938\n",
      "[1,  5800] loss: 1.011\n",
      "[1,  5820] loss: 1.215\n",
      "[1,  5840] loss: 0.796\n",
      "[1,  5860] loss: 1.459\n",
      "[1,  5880] loss: 1.172\n",
      "[1,  5900] loss: 0.765\n",
      "[1,  5920] loss: 0.813\n",
      "[1,  5940] loss: 0.676\n",
      "[1,  5960] loss: 0.704\n",
      "[1,  5980] loss: 1.180\n",
      "[1,  6000] loss: 0.995\n",
      "[1,  6020] loss: 0.813\n",
      "[1,  6040] loss: 0.858\n",
      "[1,  6060] loss: 0.707\n",
      "[1,  6080] loss: 0.912\n",
      "[1,  6100] loss: 0.823\n",
      "[1,  6120] loss: 1.303\n",
      "[1,  6140] loss: 1.021\n",
      "[1,  6160] loss: 1.053\n",
      "[1,  6180] loss: 0.971\n",
      "[1,  6200] loss: 1.129\n",
      "[1,  6220] loss: 0.807\n",
      "[1,  6240] loss: 0.846\n",
      "[1,  6260] loss: 0.772\n",
      "[1,  6280] loss: 1.101\n",
      "[1,  6300] loss: 1.010\n",
      "[1,  6320] loss: 0.833\n",
      "[1,  6340] loss: 0.744\n",
      "[1,  6360] loss: 1.008\n",
      "[1,  6380] loss: 0.859\n",
      "[1,  6400] loss: 0.801\n",
      "[1,  6420] loss: 0.615\n",
      "[1,  6440] loss: 0.936\n",
      "[1,  6460] loss: 1.063\n",
      "[1,  6480] loss: 0.900\n",
      "[1,  6500] loss: 1.012\n",
      "[1,  6520] loss: 0.661\n",
      "[1,  6540] loss: 0.909\n",
      "[1,  6560] loss: 1.170\n",
      "[1,  6580] loss: 0.896\n",
      "[1,  6600] loss: 0.863\n",
      "[1,  6620] loss: 0.913\n",
      "[1,  6640] loss: 0.982\n",
      "[1,  6660] loss: 0.770\n",
      "[1,  6680] loss: 0.995\n",
      "[1,  6700] loss: 0.751\n",
      "[1,  6720] loss: 0.736\n",
      "[1,  6740] loss: 0.785\n",
      "[1,  6760] loss: 0.852\n",
      "[1,  6780] loss: 0.915\n",
      "[1,  6800] loss: 0.813\n",
      "[1,  6820] loss: 0.835\n",
      "[1,  6840] loss: 0.833\n",
      "[1,  6860] loss: 0.977\n",
      "[1,  6880] loss: 1.003\n",
      "[1,  6900] loss: 0.704\n",
      "[1,  6920] loss: 0.817\n",
      "[1,  6940] loss: 1.024\n",
      "[1,  6960] loss: 1.027\n",
      "[1,  6980] loss: 0.972\n",
      "[1,  7000] loss: 0.848\n",
      "[1,  7020] loss: 1.139\n",
      "[1,  7040] loss: 0.896\n",
      "[1,  7060] loss: 1.478\n",
      "[1,  7080] loss: 0.983\n",
      "[1,  7100] loss: 1.072\n",
      "[1,  7120] loss: 1.156\n",
      "[1,  7140] loss: 1.335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  7160] loss: 1.155\n",
      "[1,  7180] loss: 1.346\n",
      "[1,  7200] loss: 1.340\n",
      "[1,  7220] loss: 1.096\n",
      "[1,  7240] loss: 1.057\n",
      "[1,  7260] loss: 1.263\n",
      "[1,  7280] loss: 0.943\n",
      "[1,  7300] loss: 0.838\n",
      "[1,  7320] loss: 1.168\n",
      "[1,  7340] loss: 1.022\n",
      "[1,  7360] loss: 0.887\n",
      "[1,  7380] loss: 1.197\n",
      "[1,  7400] loss: 1.152\n",
      "[1,  7420] loss: 1.023\n",
      "[1,  7440] loss: 0.733\n",
      "[1,  7460] loss: 0.926\n",
      "[1,  7480] loss: 0.702\n",
      "[1,  7500] loss: 1.004\n",
      "[1,  7520] loss: 0.958\n",
      "[1,  7540] loss: 0.907\n",
      "[1,  7560] loss: 0.667\n",
      "[1,  7580] loss: 0.735\n",
      "[1,  7600] loss: 0.683\n",
      "[1,  7620] loss: 0.616\n",
      "[1,  7640] loss: 0.914\n",
      "[1,  7660] loss: 0.876\n",
      "[1,  7680] loss: 0.818\n",
      "[1,  7700] loss: 0.874\n",
      "[1,  7720] loss: 0.675\n",
      "[1,  7740] loss: 0.886\n",
      "[1,  7760] loss: 0.813\n",
      "[1,  7780] loss: 0.642\n",
      "[1,  7800] loss: 0.823\n",
      "[1,  7820] loss: 1.260\n",
      "[1,  7840] loss: 1.226\n",
      "[1,  7860] loss: 1.022\n",
      "[1,  7880] loss: 0.971\n",
      "[1,  7900] loss: 1.059\n",
      "[1,  7920] loss: 0.914\n",
      "[1,  7940] loss: 0.866\n",
      "[1,  7960] loss: 0.951\n",
      "[1,  7980] loss: 0.756\n",
      "[1,  8000] loss: 0.621\n",
      "[1,  8020] loss: 0.896\n",
      "[1,  8040] loss: 1.531\n",
      "[1,  8060] loss: 1.135\n",
      "[1,  8080] loss: 0.913\n",
      "[1,  8100] loss: 0.938\n",
      "[1,  8120] loss: 1.264\n",
      "[1,  8140] loss: 1.135\n",
      "[1,  8160] loss: 0.998\n",
      "[1,  8180] loss: 1.255\n",
      "[1,  8200] loss: 0.949\n",
      "[1,  8220] loss: 0.980\n",
      "[1,  8240] loss: 1.013\n",
      "[1,  8260] loss: 0.882\n",
      "[1,  8280] loss: 1.084\n",
      "[1,  8300] loss: 0.776\n",
      "[1,  8320] loss: 0.983\n",
      "[1,  8340] loss: 0.934\n",
      "[1,  8360] loss: 0.785\n",
      "[1,  8380] loss: 1.004\n",
      "[1,  8400] loss: 1.169\n",
      "[1,  8420] loss: 0.862\n",
      "[1,  8440] loss: 1.244\n",
      "[1,  8460] loss: 0.836\n",
      "[1,  8480] loss: 0.820\n",
      "[1,  8500] loss: 1.604\n",
      "[1,  8520] loss: 1.645\n",
      "[1,  8540] loss: 1.621\n",
      "[1,  8560] loss: 1.632\n",
      "[1,  8580] loss: 1.415\n",
      "[1,  8600] loss: 1.137\n",
      "[1,  8620] loss: 2.915\n",
      "[1,  8640] loss: nan\n",
      "[1,  8660] loss: nan\n",
      "[1,  8680] loss: nan\n",
      "[1,  8700] loss: nan\n",
      "[1,  8720] loss: nan\n",
      "[1,  8740] loss: nan\n",
      "[1,  8760] loss: nan\n",
      "[1,  8780] loss: nan\n",
      "[1,  8800] loss: nan\n",
      "[1,  8820] loss: nan\n",
      "[1,  8840] loss: nan\n",
      "[1,  8860] loss: nan\n",
      "[1,  8880] loss: nan\n",
      "[1,  8900] loss: nan\n",
      "[1,  8920] loss: nan\n",
      "[1,  8940] loss: nan\n",
      "[1,  8960] loss: nan\n",
      "[1,  8980] loss: nan\n",
      "[1,  9000] loss: nan\n",
      "[1,  9020] loss: nan\n",
      "[1,  9040] loss: nan\n",
      "[1,  9060] loss: nan\n",
      "[1,  9080] loss: nan\n",
      "[1,  9100] loss: nan\n",
      "[1,  9120] loss: nan\n",
      "[1,  9140] loss: nan\n",
      "[1,  9160] loss: nan\n",
      "[1,  9180] loss: nan\n",
      "[1,  9200] loss: nan\n",
      "[1,  9220] loss: nan\n",
      "[1,  9240] loss: nan\n",
      "[1,  9260] loss: nan\n",
      "[1,  9280] loss: nan\n",
      "[1,  9300] loss: nan\n",
      "[1,  9320] loss: nan\n",
      "[1,  9340] loss: nan\n",
      "[1,  9360] loss: nan\n",
      "[1,  9380] loss: nan\n",
      "[1,  9400] loss: nan\n",
      "[1,  9420] loss: nan\n",
      "[1,  9440] loss: nan\n",
      "[1,  9460] loss: nan\n",
      "[1,  9480] loss: nan\n",
      "[1,  9500] loss: nan\n",
      "[1,  9520] loss: nan\n",
      "[1,  9540] loss: nan\n",
      "[1,  9560] loss: nan\n",
      "[1,  9580] loss: nan\n",
      "[1,  9600] loss: nan\n",
      "[1,  9620] loss: nan\n",
      "[1,  9640] loss: nan\n",
      "[1,  9660] loss: nan\n",
      "[1,  9680] loss: nan\n",
      "[1,  9700] loss: nan\n",
      "[1,  9720] loss: nan\n",
      "[1,  9740] loss: nan\n",
      "[1,  9760] loss: nan\n",
      "[1,  9780] loss: nan\n",
      "[1,  9800] loss: nan\n",
      "[1,  9820] loss: nan\n",
      "[1,  9840] loss: nan\n",
      "[1,  9860] loss: nan\n",
      "[2,    20] loss: nan\n",
      "[2,    40] loss: nan\n",
      "[2,    60] loss: nan\n",
      "[2,    80] loss: nan\n",
      "[2,   100] loss: nan\n",
      "[2,   120] loss: nan\n",
      "[2,   140] loss: nan\n",
      "[2,   160] loss: nan\n",
      "[2,   180] loss: nan\n",
      "[2,   200] loss: nan\n",
      "[2,   220] loss: nan\n",
      "[2,   240] loss: nan\n",
      "[2,   260] loss: nan\n",
      "[2,   280] loss: nan\n",
      "[2,   300] loss: nan\n",
      "[2,   320] loss: nan\n",
      "[2,   340] loss: nan\n",
      "[2,   360] loss: nan\n",
      "[2,   380] loss: nan\n",
      "[2,   400] loss: nan\n",
      "[2,   420] loss: nan\n",
      "[2,   440] loss: nan\n",
      "[2,   460] loss: nan\n",
      "[2,   480] loss: nan\n",
      "[2,   500] loss: nan\n",
      "[2,   520] loss: nan\n",
      "[2,   540] loss: nan\n",
      "[2,   560] loss: nan\n",
      "[2,   580] loss: nan\n",
      "[2,   600] loss: nan\n",
      "[2,   620] loss: nan\n",
      "[2,   640] loss: nan\n",
      "[2,   660] loss: nan\n",
      "[2,   680] loss: nan\n",
      "[2,   700] loss: nan\n",
      "[2,   720] loss: nan\n",
      "[2,   740] loss: nan\n",
      "[2,   760] loss: nan\n",
      "[2,   780] loss: nan\n",
      "[2,   800] loss: nan\n",
      "[2,   820] loss: nan\n",
      "[2,   840] loss: nan\n",
      "[2,   860] loss: nan\n",
      "[2,   880] loss: nan\n",
      "[2,   900] loss: nan\n",
      "[2,   920] loss: nan\n",
      "[2,   940] loss: nan\n",
      "[2,   960] loss: nan\n",
      "[2,   980] loss: nan\n",
      "[2,  1000] loss: nan\n",
      "[2,  1020] loss: nan\n",
      "[2,  1040] loss: nan\n",
      "[2,  1060] loss: nan\n",
      "[2,  1080] loss: nan\n",
      "[2,  1100] loss: nan\n",
      "[2,  1120] loss: nan\n",
      "[2,  1140] loss: nan\n",
      "[2,  1160] loss: nan\n",
      "[2,  1180] loss: nan\n",
      "[2,  1200] loss: nan\n",
      "[2,  1220] loss: nan\n",
      "[2,  1240] loss: nan\n",
      "[2,  1260] loss: nan\n",
      "[2,  1280] loss: nan\n",
      "[2,  1300] loss: nan\n",
      "[2,  1320] loss: nan\n",
      "[2,  1340] loss: nan\n",
      "[2,  1360] loss: nan\n",
      "[2,  1380] loss: nan\n",
      "[2,  1400] loss: nan\n",
      "[2,  1420] loss: nan\n",
      "[2,  1440] loss: nan\n",
      "[2,  1460] loss: nan\n",
      "[2,  1480] loss: nan\n",
      "[2,  1500] loss: nan\n",
      "[2,  1520] loss: nan\n",
      "[2,  1540] loss: nan\n",
      "[2,  1560] loss: nan\n",
      "[2,  1580] loss: nan\n",
      "[2,  1600] loss: nan\n",
      "[2,  1620] loss: nan\n",
      "[2,  1640] loss: nan\n",
      "[2,  1660] loss: nan\n",
      "[2,  1680] loss: nan\n",
      "[2,  1700] loss: nan\n",
      "[2,  1720] loss: nan\n",
      "[2,  1740] loss: nan\n",
      "[2,  1760] loss: nan\n",
      "[2,  1780] loss: nan\n",
      "[2,  1800] loss: nan\n",
<<<<<<< Updated upstream
      "[2,  1820] loss: nan\n",
      "[2,  1840] loss: nan\n",
      "[2,  1860] loss: nan\n",
      "[2,  1880] loss: nan\n",
      "[2,  1900] loss: nan\n",
      "[2,  1920] loss: nan\n",
      "[2,  1940] loss: nan\n",
      "[2,  1960] loss: nan\n",
      "[2,  1980] loss: nan\n",
      "[2,  2000] loss: nan\n",
      "[2,  2020] loss: nan\n",
      "[2,  2040] loss: nan\n",
      "[2,  2060] loss: nan\n",
      "[2,  2080] loss: nan\n",
      "[2,  2100] loss: nan\n",
      "[2,  2120] loss: nan\n",
      "[2,  2140] loss: nan\n",
      "[2,  2160] loss: nan\n",
      "[2,  2180] loss: nan\n",
      "[2,  2200] loss: nan\n",
      "[2,  2220] loss: nan\n",
      "[2,  2240] loss: nan\n",
      "[2,  2260] loss: nan\n",
      "[2,  2280] loss: nan\n",
      "[2,  2300] loss: nan\n",
      "[2,  2320] loss: nan\n",
      "[2,  2340] loss: nan\n",
      "[2,  2360] loss: nan\n",
      "[2,  2380] loss: nan\n",
      "[2,  2400] loss: nan\n",
      "[2,  2420] loss: nan\n",
      "[2,  2440] loss: nan\n",
      "[2,  2460] loss: nan\n",
      "[2,  2480] loss: nan\n",
      "[2,  2500] loss: nan\n",
      "[2,  2520] loss: nan\n",
      "[2,  2540] loss: nan\n",
      "[2,  2560] loss: nan\n",
      "[2,  2580] loss: nan\n",
      "[2,  2600] loss: nan\n",
      "[2,  2620] loss: nan\n",
      "[2,  2640] loss: nan\n",
      "[2,  2660] loss: nan\n",
      "[2,  2680] loss: nan\n",
      "[2,  2700] loss: nan\n",
      "[2,  2720] loss: nan\n",
      "[2,  2740] loss: nan\n",
      "[2,  2760] loss: nan\n",
      "[2,  2780] loss: nan\n",
      "[2,  2800] loss: nan\n",
      "[2,  2820] loss: nan\n",
      "[2,  2840] loss: nan\n",
      "[2,  2860] loss: nan\n",
      "[2,  2880] loss: nan\n",
      "[2,  2900] loss: nan\n",
      "[2,  2920] loss: nan\n",
      "[2,  2940] loss: nan\n",
      "[2,  2960] loss: nan\n",
      "[2,  2980] loss: nan\n",
      "[2,  3000] loss: nan\n",
      "[2,  3020] loss: nan\n",
      "[2,  3040] loss: nan\n",
      "[2,  3060] loss: nan\n",
      "[2,  3080] loss: nan\n",
      "[2,  3100] loss: nan\n",
      "[2,  3120] loss: nan\n",
      "[2,  3140] loss: nan\n",
      "[2,  3160] loss: nan\n",
      "[2,  3180] loss: nan\n",
      "[2,  3200] loss: nan\n",
      "[2,  3220] loss: nan\n",
      "[2,  3240] loss: nan\n",
      "[2,  3260] loss: nan\n",
      "[2,  3280] loss: nan\n",
      "[2,  3300] loss: nan\n",
      "[2,  3320] loss: nan\n",
      "[2,  3340] loss: nan\n",
      "[2,  3360] loss: nan\n",
      "[2,  3380] loss: nan\n",
      "[2,  3400] loss: nan\n",
      "[2,  3420] loss: nan\n",
      "[2,  3440] loss: nan\n",
      "[2,  3460] loss: nan\n",
      "[2,  3480] loss: nan\n",
      "[2,  3500] loss: nan\n",
      "[2,  3520] loss: nan\n",
      "[2,  3540] loss: nan\n",
      "[2,  3560] loss: nan\n",
      "[2,  3580] loss: nan\n",
      "[2,  3600] loss: nan\n",
      "[2,  3620] loss: nan\n",
      "[2,  3640] loss: nan\n",
      "[2,  3660] loss: nan\n",
      "[2,  3680] loss: nan\n",
      "[2,  3700] loss: nan\n",
      "[2,  3720] loss: nan\n",
      "[2,  3740] loss: nan\n",
      "[2,  3760] loss: nan\n",
      "[2,  3780] loss: nan\n",
      "[2,  3800] loss: nan\n",
      "[2,  3820] loss: nan\n",
      "[2,  3840] loss: nan\n",
      "[2,  3860] loss: nan\n",
      "[2,  3880] loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-d4e3f9d4684b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_labels_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlonglong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-95-280230a1dea9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch37\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 420\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
=======
      "[2,  1820] loss: nan\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "full_dataset = xTrainingSet + xValidationSet\n",
    "full_labels = yTrainingSet + yValidationSet\n",
    "\n",
    "full_dataset_batch, full_labels_batch = create_batches(full_dataset, full_labels, batch_size)\n",
    "\n",
    "net = Net(h_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "for epoch in range(max_iters):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(full_dataset_batch):\n",
    "        inputs = torch.from_numpy(data)\n",
    "        labels = full_labels_batch[i] \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.from_numpy(np.array(labels).astype(np.longlong)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 20 == 19:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict unseen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_digits = []\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    digits = []\n",
    "    \n",
    "    digits, n_digits = thresh_callback(sample)\n",
    "        \n",
    "    separated_digits.append(digits)\n",
    "    \n",
    "results = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    results.append([])\n",
    "\n",
    "row = 0\n",
    "col = 0\n",
    "    \n",
    "for i in range(int(num_test_digits / batch_size)):\n",
    "    batch = []\n",
    "    source = []\n",
    "    count = 0\n",
    "    \n",
    "    while count < batch_size:\n",
    "        batch.append(separated_digits[row][col])\n",
    "        source.append((row, col))\n",
    "        \n",
    "        if(col < len(separated_digits[row]) - 1):\n",
    "            col += 1\n",
    "        else:\n",
    "            row += 1\n",
    "            col = 0\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    batch = np.expand_dims(np.asarray(batch).astype(np.single), axis=1) \n",
    "    batch = torch.from_numpy(batch)\n",
    "    \n",
    "    output = net(batch)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    \n",
    "    for j, pred in enumerate(predicted):\n",
    "        coord = source[j]\n",
    "        results[coord[0]].append(pred.item())\n",
    "        \n",
    "for r in results:\n",
    "    pad = 5 - len(r)\n",
    "    \n",
    "    for i in range(pad):\n",
    "        r.append(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(test_dataset):\n",
    "    displayGreyWindows(sample, \"\")\n",
    "    print(results[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910101010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>017310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010101010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>467810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label\n",
       "0  910101010\n",
       "1     017310\n",
       "2      14083\n",
       "3  010101010\n",
       "4     467810"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(str(results[0]))\n",
    "\n",
    "Id = []\n",
    "label = []\n",
    "for i, x in enumerate(results):\n",
    "    Id.append(i)\n",
    "    string = ''.join([str(elem) for elem in x])\n",
    "    label.append(string)\n",
    "\"\"\"\n",
    "print(Id[0],label[0])\n",
    "print(df[\"Label\"])\n",
    "\"\"\"\n",
    "data={\"Label\":label}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv('sample.csv', index_label = \"Id\")\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracies = []\n",
    "validation_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, h_layers):\n",
    "        self.num_layers = len(h_layers)\n",
    "        self.h_layers = h_layers      \n",
    "        self.convs = []\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(3 * 3 * 256, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        #print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        #print(out.shape)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 2.306\n",
      "[1,    40] loss: 2.282\n",
      "[1,    60] loss: 2.291\n",
      "[1,    80] loss: 2.267\n",
      "[1,   100] loss: 2.270\n",
      "[1,   120] loss: 2.238\n",
      "[1,   140] loss: 2.216\n",
      "[1,   160] loss: 2.167\n",
      "[1,   180] loss: 2.091\n",
      "[1,   200] loss: 1.949\n",
      "[1,   220] loss: 1.813\n",
      "[1,   240] loss: 1.663\n",
      "[1,   260] loss: 1.454\n",
      "[1,   280] loss: 1.296\n",
      "[1,   300] loss: 1.243\n",
      "[1,   320] loss: 1.070\n",
      "[1,   340] loss: 1.012\n",
      "[1,   360] loss: 0.866\n",
      "[1,   380] loss: 0.792\n",
      "[1,   400] loss: 0.774\n",
      "[1,   420] loss: 0.647\n",
      "[1,   440] loss: 0.703\n",
      "[1,   460] loss: 0.689\n",
      "[1,   480] loss: 0.501\n",
      "[1,   500] loss: 0.459\n",
      "[1,   520] loss: 0.584\n",
      "[1,   540] loss: 0.554\n",
      "[1,   560] loss: 0.532\n",
      "[1,   580] loss: 0.484\n",
      "[1,   600] loss: 0.463\n",
      "[1,   620] loss: 0.531\n",
      "[1,   640] loss: 0.410\n",
      "[1,   660] loss: 0.392\n",
      "[1,   680] loss: 0.356\n",
      "[1,   700] loss: 0.352\n",
      "[1,   720] loss: 0.310\n",
      "[1,   740] loss: 0.322\n",
      "[1,   760] loss: 0.312\n",
      "[1,   780] loss: 0.231\n",
      "[1,   800] loss: 0.339\n",
      "[1,   820] loss: 0.279\n",
      "[1,   840] loss: 0.243\n",
      "[1,   860] loss: 0.322\n",
      "[1,   880] loss: 0.273\n",
      "[1,   900] loss: 0.234\n",
      "[1,   920] loss: 0.233\n",
      "[1,   940] loss: 0.259\n",
      "[1,   960] loss: 0.220\n",
      "[1,   980] loss: 0.247\n",
      "[1,  1000] loss: 0.349\n",
      "[1,  1020] loss: 0.299\n",
      "[1,  1040] loss: 0.263\n",
      "[1,  1060] loss: 0.219\n",
      "[1,  1080] loss: 0.191\n",
      "[1,  1100] loss: 0.247\n",
      "[1,  1120] loss: 0.178\n",
      "[1,  1140] loss: 0.271\n",
      "[1,  1160] loss: 0.210\n",
      "[1,  1180] loss: 0.243\n",
      "[1,  1200] loss: 0.292\n",
      "[1,  1220] loss: 0.206\n",
      "[1,  1240] loss: 0.222\n",
      "[1,  1260] loss: 0.151\n",
      "[1,  1280] loss: 0.180\n",
      "[1,  1300] loss: 0.337\n",
      "[1,  1320] loss: 0.215\n",
      "[1,  1340] loss: 0.201\n",
      "[1,  1360] loss: 0.163\n",
      "[1,  1380] loss: 0.227\n",
      "[1,  1400] loss: 0.126\n",
      "[1,  1420] loss: 0.177\n",
      "[1,  1440] loss: 0.269\n",
      "[1,  1460] loss: 0.224\n",
      "[1,  1480] loss: 0.168\n",
      "[1,  1500] loss: 0.148\n",
      "[1,  1520] loss: 0.242\n",
      "[1,  1540] loss: 0.128\n",
      "[1,  1560] loss: 0.132\n",
      "[1,  1580] loss: 0.263\n",
      "[1,  1600] loss: 0.276\n",
      "[1,  1620] loss: 0.182\n",
      "[1,  1640] loss: 0.155\n",
      "[1,  1660] loss: 0.158\n",
      "[1,  1680] loss: 0.113\n",
      "[1,  1700] loss: 0.253\n",
      "[1,  1720] loss: 0.166\n",
      "[1,  1740] loss: 0.168\n",
      "[1,  1760] loss: 0.188\n",
      "[1,  1780] loss: 0.154\n",
      "[1,  1800] loss: 0.118\n",
      "[1,  1820] loss: 0.209\n",
      "[1,  1840] loss: 0.218\n",
      "[1,  1860] loss: 0.107\n",
      "[1,  1880] loss: 0.105\n",
      "[1,  1900] loss: 0.096\n",
      "[1,  1920] loss: 0.229\n",
      "[1,  1940] loss: 0.145\n",
      "[1,  1960] loss: 0.141\n",
      "[1,  1980] loss: 0.170\n",
      "[1,  2000] loss: 0.124\n",
      "[1,  2020] loss: 0.127\n",
      "[1,  2040] loss: 0.238\n",
      "[1,  2060] loss: 0.139\n",
      "[1,  2080] loss: 0.107\n",
      "[1,  2100] loss: 0.148\n",
      "[1,  2120] loss: 0.145\n",
      "[1,  2140] loss: 0.190\n",
      "[1,  2160] loss: 0.140\n",
      "[1,  2180] loss: 0.137\n",
      "[1,  2200] loss: 0.100\n",
      "[1,  2220] loss: 0.145\n",
      "[1,  2240] loss: 0.172\n",
      "[1,  2260] loss: 0.104\n",
      "[1,  2280] loss: 0.142\n",
      "[1,  2300] loss: 0.132\n",
      "[1,  2320] loss: 0.152\n",
      "[1,  2340] loss: 0.185\n",
      "[1,  2360] loss: 0.122\n",
      "[1,  2380] loss: 0.108\n",
      "[1,  2400] loss: 0.149\n",
      "[1,  2420] loss: 0.110\n",
      "[1,  2440] loss: 0.099\n",
      "[1,  2460] loss: 0.129\n",
      "[1,  2480] loss: 0.125\n",
      "[1,  2500] loss: 0.123\n",
      "[1,  2520] loss: 0.127\n",
      "[1,  2540] loss: 0.148\n",
      "[1,  2560] loss: 0.116\n",
      "[1,  2580] loss: 0.104\n",
      "[1,  2600] loss: 0.133\n",
      "[1,  2620] loss: 0.150\n",
      "[1,  2640] loss: 0.141\n",
      "[1,  2660] loss: 0.117\n",
      "[1,  2680] loss: 0.184\n",
      "[1,  2700] loss: 0.146\n",
      "[1,  2720] loss: 0.157\n",
      "[1,  2740] loss: 0.190\n",
      "[1,  2760] loss: 0.211\n",
      "[1,  2780] loss: 0.170\n",
      "[1,  2800] loss: 0.105\n",
      "[1,  2820] loss: 0.097\n",
      "[1,  2840] loss: 0.133\n",
      "[1,  2860] loss: 0.132\n",
      "[1,  2880] loss: 0.071\n",
      "[1,  2900] loss: 0.096\n",
      "[1,  2920] loss: 0.113\n",
      "[1,  2940] loss: 0.117\n",
      "[1,  2960] loss: 0.140\n",
      "[1,  2980] loss: 0.071\n",
      "[1,  3000] loss: 0.087\n",
      "[1,  3020] loss: 0.154\n",
      "[1,  3040] loss: 0.159\n",
      "[1,  3060] loss: 0.109\n",
      "[1,  3080] loss: 0.107\n",
      "[1,  3100] loss: 0.138\n",
      "[1,  3120] loss: 0.105\n",
      "[1,  3140] loss: 0.072\n",
      "[1,  3160] loss: 0.163\n",
      "[1,  3180] loss: 0.136\n",
      "[1,  3200] loss: 0.171\n",
      "[1,  3220] loss: 0.153\n",
      "[1,  3240] loss: 0.102\n",
      "[1,  3260] loss: 0.153\n",
      "[1,  3280] loss: 0.057\n",
      "[1,  3300] loss: 0.205\n",
      "[1,  3320] loss: 0.105\n",
      "[1,  3340] loss: 0.098\n",
      "[1,  3360] loss: 0.164\n",
      "[1,  3380] loss: 0.090\n",
      "[1,  3400] loss: 0.184\n",
      "[1,  3420] loss: 0.083\n",
      "[1,  3440] loss: 0.124\n",
      "[1,  3460] loss: 0.120\n",
      "[1,  3480] loss: 0.106\n",
      "[1,  3500] loss: 0.091\n",
      "[1,  3520] loss: 0.096\n",
      "[1,  3540] loss: 0.089\n",
      "[1,  3560] loss: 0.101\n",
      "[1,  3580] loss: 0.123\n",
      "[1,  3600] loss: 0.118\n",
      "[1,  3620] loss: 0.109\n",
      "[1,  3640] loss: 0.111\n",
      "[1,  3660] loss: 0.135\n",
      "[1,  3680] loss: 0.093\n",
      "[1,  3700] loss: 0.126\n",
      "[1,  3720] loss: 0.136\n",
      "[1,  3740] loss: 0.128\n",
      "[1,  3760] loss: 0.093\n",
      "[1,  3780] loss: 0.065\n",
      "[1,  3800] loss: 0.123\n",
      "[1,  3820] loss: 0.065\n",
      "[1,  3840] loss: 0.075\n",
      "[1,  3860] loss: 0.128\n",
      "[1,  3880] loss: 0.159\n",
      "[1,  3900] loss: 0.107\n",
      "[1,  3920] loss: 0.104\n",
      "[1,  3940] loss: 0.073\n",
      "[1,  3960] loss: 0.053\n",
      "[1,  3980] loss: 0.103\n",
      "[1,  4000] loss: 0.107\n",
      "[1,  4020] loss: 0.079\n",
      "[1,  4040] loss: 0.048\n",
      "[1,  4060] loss: 0.084\n",
      "[1,  4080] loss: 0.075\n",
      "[1,  4100] loss: 0.111\n",
      "[1,  4120] loss: 0.070\n",
      "[1,  4140] loss: 0.060\n",
      "[1,  4160] loss: 0.076\n",
      "[1,  4180] loss: 0.082\n",
      "[1,  4200] loss: 0.104\n",
      "[1,  4220] loss: 0.069\n",
      "[1,  4240] loss: 0.073\n",
      "[1,  4260] loss: 0.107\n",
      "[1,  4280] loss: 0.087\n",
      "[1,  4300] loss: 0.064\n",
      "[1,  4320] loss: 0.088\n",
      "[1,  4340] loss: 0.071\n",
      "[1,  4360] loss: 0.087\n",
      "[1,  4380] loss: 0.084\n",
      "[1,  4400] loss: 0.135\n",
      "[1,  4420] loss: 0.097\n",
      "[1,  4440] loss: 0.098\n",
      "[1,  4460] loss: 0.073\n",
      "[1,  4480] loss: 0.121\n",
      "[1,  4500] loss: 0.108\n",
      "[1,  4520] loss: 0.090\n",
      "[1,  4540] loss: 0.087\n",
      "[1,  4560] loss: 0.092\n",
      "[1,  4580] loss: 0.062\n",
      "[1,  4600] loss: 0.088\n",
      "[1,  4620] loss: 0.089\n",
      "[1,  4640] loss: 0.084\n",
      "[1,  4660] loss: 0.060\n",
      "[1,  4680] loss: 0.091\n",
      "[1,  4700] loss: 0.044\n",
      "[1,  4720] loss: 0.149\n",
      "[1,  4740] loss: 0.134\n",
      "[1,  4760] loss: 0.101\n",
      "[1,  4780] loss: 0.078\n",
      "[1,  4800] loss: 0.072\n",
      "[1,  4820] loss: 0.054\n",
      "[1,  4840] loss: 0.099\n",
      "[1,  4860] loss: 0.107\n",
      "[1,  4880] loss: 0.070\n",
      "[1,  4900] loss: 0.035\n",
      "[1,  4920] loss: 0.093\n",
      "[1,  4940] loss: 0.094\n",
      "[1,  4960] loss: 0.074\n",
      "[1,  4980] loss: 0.105\n",
      "[1,  5000] loss: 0.101\n",
      "[1,  5020] loss: 0.063\n",
      "[1,  5040] loss: 0.101\n",
      "[1,  5060] loss: 0.113\n",
      "[1,  5080] loss: 0.118\n",
      "[1,  5100] loss: 0.132\n",
      "[1,  5120] loss: 0.098\n",
      "[1,  5140] loss: 0.073\n",
      "[1,  5160] loss: 0.087\n",
      "[1,  5180] loss: 0.151\n",
      "[1,  5200] loss: 0.087\n",
      "[1,  5220] loss: 0.065\n",
      "[1,  5240] loss: 0.068\n",
      "[1,  5260] loss: 0.103\n",
      "[1,  5280] loss: 0.078\n",
      "[1,  5300] loss: 0.109\n",
      "[1,  5320] loss: 0.101\n",
      "[1,  5340] loss: 0.073\n",
      "[1,  5360] loss: 0.090\n",
      "[1,  5380] loss: 0.106\n",
      "[1,  5400] loss: 0.083\n",
      "[1,  5420] loss: 0.099\n",
      "[1,  5440] loss: 0.075\n",
      "[1,  5460] loss: 0.084\n",
      "[1,  5480] loss: 0.078\n",
      "[1,  5500] loss: 0.029\n",
      "[1,  5520] loss: 0.038\n",
      "[1,  5540] loss: 0.110\n",
      "[1,  5560] loss: 0.053\n",
      "[1,  5580] loss: 0.063\n",
      "[1,  5600] loss: 0.143\n",
      "[1,  5620] loss: 0.070\n",
      "[1,  5640] loss: 0.092\n",
      "[1,  5660] loss: 0.057\n",
      "[1,  5680] loss: 0.061\n",
      "[1,  5700] loss: 0.084\n",
      "[1,  5720] loss: 0.027\n",
      "[1,  5740] loss: 0.073\n",
      "[1,  5760] loss: 0.034\n",
      "[1,  5780] loss: 0.105\n",
      "[1,  5800] loss: 0.053\n",
      "[1,  5820] loss: 0.068\n",
      "[1,  5840] loss: 0.047\n",
      "[1,  5860] loss: 0.092\n",
      "[1,  5880] loss: 0.071\n",
      "[1,  5900] loss: 0.059\n",
      "[1,  5920] loss: 0.067\n",
      "[1,  5940] loss: 0.071\n",
      "[1,  5960] loss: 0.038\n",
      "[1,  5980] loss: 0.054\n",
      "[1,  6000] loss: 0.088\n",
      "[1,  6020] loss: 0.063\n",
      "[1,  6040] loss: 0.046\n",
      "[1,  6060] loss: 0.071\n",
      "[1,  6080] loss: 0.092\n",
      "[1,  6100] loss: 0.058\n",
      "[1,  6120] loss: 0.062\n",
      "[1,  6140] loss: 0.044\n",
      "[1,  6160] loss: 0.034\n",
      "[1,  6180] loss: 0.051\n",
      "[1,  6200] loss: 0.085\n",
      "[1,  6220] loss: 0.063\n",
      "[1,  6240] loss: 0.106\n",
      "[1,  6260] loss: 0.067\n",
      "[1,  6280] loss: 0.137\n",
      "[1,  6300] loss: 0.101\n",
      "[1,  6320] loss: 0.105\n",
      "[1,  6340] loss: 0.080\n",
      "[1,  6360] loss: 0.070\n",
      "[1,  6380] loss: 0.090\n",
      "[1,  6400] loss: 0.128\n",
      "[1,  6420] loss: 0.107\n",
      "[1,  6440] loss: 0.081\n",
      "[1,  6460] loss: 0.059\n",
      "[1,  6480] loss: 0.034\n",
      "[1,  6500] loss: 0.062\n",
      "[1,  6520] loss: 0.025\n",
      "[1,  6540] loss: 0.080\n",
      "[1,  6560] loss: 0.132\n",
      "[1,  6580] loss: 0.071\n",
      "[1,  6600] loss: 0.079\n",
      "[1,  6620] loss: 0.061\n",
      "[1,  6640] loss: 0.079\n",
      "[1,  6660] loss: 0.051\n",
      "[1,  6680] loss: 0.068\n",
      "[1,  6700] loss: 0.089\n",
      "[1,  6720] loss: 0.091\n",
      "[1,  6740] loss: 0.085\n",
      "[1,  6760] loss: 0.081\n",
      "[1,  6780] loss: 0.040\n",
      "[1,  6800] loss: 0.038\n",
      "[1,  6820] loss: 0.070\n",
      "[1,  6840] loss: 0.111\n",
      "[1,  6860] loss: 0.142\n",
      "[1,  6880] loss: 0.081\n",
      "[1,  6900] loss: 0.116\n",
      "[1,  6920] loss: 0.069\n",
      "[1,  6940] loss: 0.099\n",
      "[1,  6960] loss: 0.083\n",
      "[1,  6980] loss: 0.063\n",
      "[1,  7000] loss: 0.077\n",
      "[1,  7020] loss: 0.087\n",
      "[1,  7040] loss: 0.095\n",
      "[1,  7060] loss: 0.061\n",
      "[1,  7080] loss: 0.059\n",
      "[1,  7100] loss: 0.064\n",
      "[1,  7120] loss: 0.078\n",
      "[1,  7140] loss: 0.088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  7160] loss: 0.090\n",
      "[1,  7180] loss: 0.063\n",
      "[1,  7200] loss: 0.067\n",
      "[1,  7220] loss: 0.096\n",
      "[1,  7240] loss: 0.097\n",
      "[1,  7260] loss: 0.035\n",
      "[1,  7280] loss: 0.091\n",
      "[1,  7300] loss: 0.059\n",
      "[1,  7320] loss: 0.034\n",
      "[1,  7340] loss: 0.055\n",
      "[1,  7360] loss: 0.072\n",
      "[1,  7380] loss: 0.070\n",
      "[1,  7400] loss: 0.097\n",
      "[1,  7420] loss: 0.064\n",
      "[1,  7440] loss: 0.064\n",
      "[1,  7460] loss: 0.087\n",
      "[1,  7480] loss: 0.089\n",
      "[1,  7500] loss: 0.100\n",
      "[1,  7520] loss: 0.064\n",
      "[1,  7540] loss: 0.071\n",
      "[1,  7560] loss: 0.088\n",
      "[1,  7580] loss: 0.072\n",
      "[1,  7600] loss: 0.088\n",
      "[1,  7620] loss: 0.039\n",
      "[1,  7640] loss: 0.075\n",
      "[1,  7660] loss: 0.075\n",
      "[1,  7680] loss: 0.052\n",
      "[1,  7700] loss: 0.102\n",
      "[1,  7720] loss: 0.061\n",
      "[1,  7740] loss: 0.041\n",
      "[1,  7760] loss: 0.028\n",
      "[1,  7780] loss: 0.075\n",
      "[1,  7800] loss: 0.071\n",
      "[1,  7820] loss: 0.031\n",
      "[1,  7840] loss: 0.050\n",
      "[1,  7860] loss: 0.048\n",
      "[1,  7880] loss: 0.104\n",
      "[1,  7900] loss: 0.091\n"
     ]
    }
   ],
   "source": [
    "training_dataset_batch, training_labels_batch = create_batches(xTrainingSet, yTrainingSet, batch_size)\n",
    "validation_dataset_batch, validation_labels_batch = create_batches(xValidationSet, yValidationSet, batch_size)\n",
    "\n",
    "# model hyper-parameters\n",
    "h_layers = ([batch_size, 64])\n",
    "max_iters = 1\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "\n",
    "dont_run = False\n",
    "# train neural network\n",
    "\n",
    "\n",
    "\n",
    "if not dont_run:\n",
    "    net = Net(h_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    for epoch in range(max_iters):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(training_dataset_batch):\n",
    "            inputs = torch.from_numpy(data)\n",
    "            labels = training_labels_batch[i] \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, torch.from_numpy(np.array(labels).astype(np.longlong)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 20 == 19:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    # test validation set on model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(training_dataset_batch):\n",
    "            images = torch.from_numpy(data)\n",
    "            labels = training_labels_batch[i]\n",
    "            labels = torch.from_numpy(np.array(labels).astype(np.longlong))\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if (int(predicted[0]) != int(labels[0])):\n",
    "                None\n",
    "\n",
    "        training_accuracies.append(100 * correct / total)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validation_dataset_batch):\n",
    "            images = torch.from_numpy(data)\n",
    "            labels = validation_labels_batch[i]\n",
    "            labels = torch.from_numpy(np.array(labels).astype(np.longlong))\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if (int(predicted[0]) != int(labels[0])):\n",
    "                None\n",
    "\n",
    "        validation_accuracies.append(100 * correct / total)\n",
    "\n",
    "    #print(f\"validation set accuracy ({total} samples): {(100 * correct / total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85.81012658227849, 95.95681310498883, 97.82278481012658, 98.71481757259866, 98.37006701414744, 98.20774385703649]\n",
      "[85.76142963514519, 95.93149664929263, 97.80670141474312, 98.68830975428146, 98.30290394638868, 98.17781087118392]\n"
     ]
    }
   ],
   "source": [
    "print(training_accuracies)\n",
    "print(validation_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(10, 7))\n",
    "fig.autofmt_xdate()\n",
    "legend = np.zeros(6,dtype=object)\n",
    "#for i in range(2):\n",
    "x_axis = np.arange(0,6)\n",
    "y_axis = training_accuracies\n",
    "plt.scatter(x_axis+1, y_axis)\n",
    "plt.plot(x_axis+1, y_axis, linestyle='solid')\n",
    "y_axis = validation_accuracies\n",
    "plt.scatter(x_axis+1, y_axis)\n",
    "plt.plot(x_axis+1, y_axis, linestyle='solid')\n",
    "\n",
    "legend = (\"Training\",\"Validation\")\n",
    "ax.legend(legend)\n",
    "plt.xlabel('Convolutional layers',fontsize=15)\n",
    "plt.ylabel('Accuracy (%)',fontsize=15)\n",
    "plt.title(\"MNIST dataset accuracy with changing amount of convolution layers\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
