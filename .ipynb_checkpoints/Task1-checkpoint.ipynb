{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBGRImage(path):\n",
    "    image = BGR(cv.imread(path))\n",
    "    return image\n",
    "\n",
    "def loadGreyImage(path):\n",
    "    image = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
    "    return image\n",
    "\n",
    "def BGR(image):\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "def displayGreyImage(image, imageName):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "\n",
    "def displayGreyWindows(image, imageName):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def displayBGRImage(image, imageName, size):\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(image)\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def displayBGRImageLarge(image, imageName):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    plt.imshow(image)\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def imageSideBySide(images, imageNames,size):\n",
    "    row = np.ceil(len(images)/20)\n",
    "    fig=plt.figure(figsize=(size, size/2))\n",
    "    for i, image in enumerate(images):\n",
    "        fig.add_subplot(row, 20, i+1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(imageNames[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_dataset', 'train_dataset', 'train_labels']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 25, 26\n",
    "size = 14, 12\n",
    "\n",
    "f = h5py.File('MNIST_synthetic.h5', 'r')\n",
    "\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.squeeze(np.array(f[\"train_dataset\"])).copy()\n",
    "train_labels = np.squeeze(np.array(f[\"train_labels\"])).copy()\n",
    "test_dataset = np.squeeze(np.array(f[\"test_dataset\"])).copy()\n",
    "\n",
    "singulars_digits = []\n",
    "singulars_labels = []\n",
    "\n",
    "doubles_digits = []\n",
    "doubles_labels = []\n",
    "\n",
    "triples_digits = []\n",
    "triples_labels = []\n",
    "\n",
    "quadruples_digits = []\n",
    "quadruples_labels = []\n",
    "\n",
    "quintuples_digits = []\n",
    "quintuples_labels = []\n",
    "\n",
    "\n",
    "for i, labels in enumerate(train_labels):\n",
    "    if labels[1] == 10:\n",
    "        singulars_digits.append(train_dataset[i])\n",
    "        singulars_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[1] != 10 and labels[2] == 10:\n",
    "        doubles_digits.append(train_dataset[i])\n",
    "        doubles_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[2] != 10 and labels[3] == 10:\n",
    "        triples_digits.append(train_dataset[i])\n",
    "        triples_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[3] != 10 and labels[4] == 10:\n",
    "        quadruples_digits.append(train_dataset[i])\n",
    "        quadruples_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[4] != 10:\n",
    "        quintuples_digits.append(train_dataset[i])\n",
    "        quintuples_labels.append(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "singulars_digits = np.array(singulars_digits)     \n",
    "doubles_digits = np.array(doubles_digits)  \n",
    "triples_digits = np.array(triples_digits)    \n",
    "quadruples_digits = np.array(quadruples_digits)    \n",
    "quintuples_digits = np.array(quintuples_digits)    \n",
    "\n",
    "singulars_labels = np.array(singulars_labels).T[0]\n",
    "doubles_labels = np.array(doubles_labels).T[0:2].T\n",
    "triples_label = np.array(triples_labels).T[0:3].T\n",
    "quadruples_label = np.array(quadruples_labels).T[0:4].T\n",
    "quintuples_label = np.array(quintuples_labels).T[0:5].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_singulars_dataset = []\n",
    "final_singulars_labels = list(singulars_labels)\n",
    "for i, digit in enumerate(singulars_digits):\n",
    "    final_singulars_dataset.append(digit[26:39,26:39])\n",
    "    \n",
    "final_doubles_dataset = []\n",
    "final_doubles_labels = []\n",
    "for i, digit in enumerate(doubles_digits):\n",
    "    final_doubles_dataset.append(digit[26:39,20:33])\n",
    "    final_doubles_dataset.append(digit[26:39,32:45])\n",
    "    final_doubles_labels.append(doubles_labels[i][0])\n",
    "    final_doubles_labels.append(doubles_labels[i][1])\n",
    "\n",
    "final_triples_dataset = []\n",
    "final_triples_labels = []\n",
    "for i, digit in enumerate(triples_digits):\n",
    "    final_triples_dataset.append(digit[26:39,14:27])\n",
    "    final_triples_dataset.append(digit[26:39,26:39])\n",
    "    final_triples_dataset.append(digit[26:39,38:51])\n",
    "    final_triples_labels.append(triples_labels[i][0])\n",
    "    final_triples_labels.append(triples_labels[i][1])\n",
    "    final_triples_labels.append(triples_labels[i][2])\n",
    "    \n",
    "final_quadruples_dataset = []\n",
    "final_quadruples_labels = []\n",
    "for i, digit in enumerate(quadruples_digits):\n",
    "    final_quadruples_dataset.append(digit[26:39,8:21])\n",
    "    final_quadruples_dataset.append(digit[26:39,20:33])\n",
    "    final_quadruples_dataset.append(digit[26:39,32:45])\n",
    "    final_quadruples_dataset.append(digit[26:39,44:57])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][0])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][1])    \n",
    "    final_quadruples_labels.append(quadruples_labels[i][2])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][3]) \n",
    "    \n",
    "final_quintuples_dataset = []\n",
    "final_quintuples_labels = []\n",
    "for i, digit in enumerate(quintuples_digits):\n",
    "    final_quintuples_dataset.append(digit[26:39,2:15])\n",
    "    final_quintuples_dataset.append(digit[26:39,14:27])\n",
    "    final_quintuples_dataset.append(digit[26:39,26:39])\n",
    "    final_quintuples_dataset.append(digit[26:39,38:51])\n",
    "    final_quintuples_dataset.append(digit[26:39,50:63])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][0])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][1])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][2])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][3])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = final_singulars_dataset + final_doubles_dataset + final_triples_dataset + final_quadruples_dataset + final_quintuples_dataset\n",
    "merged_labels = final_singulars_labels + final_doubles_labels + final_triples_labels + final_quadruples_labels + final_quintuples_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 0's in train dataset: 16379\n",
      "number of 1's in train dataset: 18819\n",
      "number of 2's in train dataset: 17095\n",
      "number of 3's in train dataset: 17220\n",
      "number of 4's in train dataset: 16393\n",
      "number of 5's in train dataset: 15275\n",
      "number of 6's in train dataset: 16401\n",
      "number of 7's in train dataset: 17611\n",
      "number of 8's in train dataset: 16175\n",
      "number of 9's in train dataset: 16518\n",
      "total number of digits in train dataset: 167886\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for digit in range(10):\n",
    "    digit_count = list(merged_labels).count(digit)\n",
    "    print(f\"number of {digit}'s in train dataset: {digit_count}\")\n",
    "    count += digit_count\n",
    "\n",
    "print(\"total number of digits in train dataset:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(merged_dataset)))\n",
    "digitsValidationSetSize = int(len(merged_labels) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(1):\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, digit in enumerate(merged_dataset[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(digit)\n",
    "        yValidationSet.append(merged_labels[index+(foldIndex*digitsValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    start = len(xValidationSet)\n",
    " \n",
    "    for i, digit in enumerate(merged_dataset[start:]):\n",
    "        xTrainingSet.append(digit)\n",
    "        yTrainingSet.append(merged_labels[i+start])\n",
    "\n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_digits(train_dataset, size):\n",
    "    i,j = np.where(train_dataset[:,:]!=0)\n",
    "    #print(i)\n",
    "    bottom_bound = np.min(i)\n",
    "    top_bound = np.max(i)\n",
    "    left_bound = np.min(j)\n",
    "    right_bound = np.max(j)\n",
    "    #print(top_bound, bottom_bound)\n",
    "\n",
    "    h = top_bound-bottom_bound\n",
    "    w = right_bound-left_bound\n",
    "\n",
    "    bounded_train_dataset = train_dataset[bottom_bound-1:top_bound+2,left_bound-1:right_bound+2]\n",
    "    bounded_train_dataset[bounded_train_dataset>30] = 255\n",
    "    bounded_train_dataset[bounded_train_dataset!=255] = 0\n",
    "    #ret, thresh = cv2.threshold(bounded_train_dataset, 30, 255, 0)\n",
    "    seg = np.where(np.any(bounded_train_dataset, axis=0)==0)\n",
    "\n",
    "    #print(seg)\n",
    "    seg_list = np.asarray(seg)\n",
    "    seg_list = seg_list[0]\n",
    "    #print(seg_list)\n",
    "    from statistics import stdev\n",
    "\n",
    "    if len(seg_list)>2:\n",
    "        # create a list of the gaps between the consecutive values\n",
    "        gaps = [y - x for x, y in zip(seg_list[:-1], seg_list[1:])]\n",
    "        # have python calculate the standard deviation for the gaps\n",
    "        sd = stdev(gaps)\n",
    "\n",
    "        # create a list of lists, put the first value of the source data in the first\n",
    "        lists = [[seg_list[0]]]\n",
    "        for x in seg_list[1:]:\n",
    "            # if the gap from the current item to the previous is more than 1 SD\n",
    "            # Note: the previous item is the last item in the last list\n",
    "            # Note: the '> 1' is the part you'd modify to make it stricter or more relaxed\n",
    "            if (x - lists[-1][-1]) / sd > 0.8:\n",
    "                # then start a new list\n",
    "                lists.append([])\n",
    "            # add the current item to the last list in the list\n",
    "            lists[-1].append(x)\n",
    "\n",
    "        splits = np.asarray([np.ceil(np.mean(lists[i])) for i in range(len(lists))]).astype(int)\n",
    "\n",
    "    else:\n",
    "        splits = np.asarray(seg_list)\n",
    "    #print(splits)\n",
    "\n",
    "    n_digits = len(splits)-1\n",
    "    #digits = np.zeros(n_digits, )\n",
    "    digits = []\n",
    "    for i in range(n_digits):\n",
    "        temp = bounded_train_dataset[:,splits[i]:splits[i+1]]\n",
    "        # if temp is less than recommeded size first pad on left and then on both sides\n",
    "        temp_padded = temp.copy()\n",
    "        if (temp.shape[0] != size) or  (temp.shape[1] != size):\n",
    "            diff_y = size-temp.shape[0]\n",
    "            split_diff_y = diff_y//2\n",
    "            remainder_diff_y = diff_y%2\n",
    "            diff_x = size-temp.shape[1]\n",
    "            split_diff_x = diff_x//2\n",
    "            remainder_diff_x = diff_x%2\n",
    "            #print(diff_y)\n",
    "            #print(split_diff_y)\n",
    "            temp_padded = np.pad(temp, ((split_diff_y+remainder_diff_y,split_diff_y),(split_diff_x+remainder_diff_x,split_diff_x)))\n",
    "\n",
    "        digits.append(temp_padded)\n",
    "    return digits, n_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect at index: 367\n",
      "incorrect at index: 381\n",
      "incorrect at index: 810\n",
      "incorrect at index: 976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-bad9dfd1db91>:37: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if (x - lists[-1][-1]) / sd > 0.8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect at index: 1232\n",
      "incorrect at index: 1351\n",
      "incorrect at index: 1606\n",
      "incorrect at index: 2208\n",
      "incorrect at index: 2684\n",
      "incorrect at index: 3134\n",
      "incorrect at index: 3164\n",
      "incorrect at index: 3172\n",
      "incorrect at index: 3282\n",
      "incorrect at index: 3439\n",
      "incorrect at index: 3524\n",
      "incorrect at index: 3771\n",
      "incorrect at index: 3838\n",
      "incorrect at index: 3883\n",
      "incorrect at index: 4011\n",
      "incorrect at index: 4333\n",
      "incorrect at index: 4447\n",
      "incorrect at index: 4501\n",
      "incorrect at index: 4639\n",
      "incorrect at index: 4747\n",
      "incorrect at index: 5243\n",
      "incorrect at index: 5419\n",
      "incorrect at index: 5442\n",
      "incorrect at index: 5772\n",
      "incorrect at index: 5930\n",
      "incorrect at index: 6134\n",
      "incorrect at index: 6949\n",
      "incorrect at index: 7118\n",
      "incorrect at index: 7170\n",
      "incorrect at index: 7271\n",
      "incorrect at index: 7514\n",
      "incorrect at index: 8089\n",
      "incorrect at index: 8860\n",
      "incorrect at index: 8941\n",
      "incorrect at index: 9038\n",
      "incorrect at index: 9176\n",
      "incorrect at index: 9419\n",
      "incorrect at index: 9426\n",
      "incorrect at index: 9580\n",
      "incorrect at index: 9768\n",
      "incorrect at index: 9997\n",
      "incorrect at index: 10702\n",
      "incorrect at index: 11475\n",
      "incorrect at index: 11561\n",
      "incorrect at index: 11622\n",
      "incorrect at index: 11810\n",
      "incorrect at index: 11919\n",
      "incorrect at index: 12806\n",
      "incorrect at index: 13280\n",
      "incorrect at index: 13360\n",
      "incorrect at index: 13514\n",
      "incorrect at index: 13709\n",
      "incorrect at index: 14150\n",
      "incorrect at index: 14552\n",
      "incorrect at index: 14931\n",
      "incorrect at index: 15844\n",
      "incorrect at index: 16240\n",
      "incorrect at index: 16366\n",
      "incorrect at index: 16384\n",
      "incorrect at index: 16868\n",
      "incorrect at index: 16909\n",
      "incorrect at index: 17030\n",
      "incorrect at index: 17540\n",
      "incorrect at index: 17573\n",
      "incorrect at index: 17658\n",
      "incorrect at index: 18122\n",
      "incorrect at index: 18127\n",
      "incorrect at index: 18460\n",
      "incorrect at index: 19104\n",
      "incorrect at index: 19332\n",
      "incorrect at index: 19660\n",
      "incorrect at index: 19822\n",
      "incorrect at index: 20029\n",
      "incorrect at index: 20479\n",
      "incorrect at index: 20676\n",
      "incorrect at index: 20764\n",
      "incorrect at index: 21153\n",
      "incorrect at index: 21814\n",
      "incorrect at index: 21854\n",
      "incorrect at index: 21980\n",
      "incorrect at index: 22025\n",
      "incorrect at index: 22369\n",
      "incorrect at index: 22596\n",
      "incorrect at index: 22898\n",
      "incorrect at index: 23044\n",
      "incorrect at index: 23211\n",
      "incorrect at index: 23545\n",
      "incorrect at index: 23684\n",
      "incorrect at index: 23850\n",
      "incorrect at index: 23924\n",
      "incorrect at index: 24015\n",
      "incorrect at index: 24026\n",
      "incorrect at index: 24518\n",
      "incorrect at index: 24650\n",
      "incorrect at index: 24839\n",
      "incorrect at index: 25190\n",
      "incorrect at index: 25233\n",
      "incorrect at index: 25273\n",
      "incorrect at index: 25862\n",
      "incorrect at index: 26042\n",
      "incorrect at index: 26094\n",
      "incorrect at index: 26601\n",
      "incorrect at index: 27040\n",
      "incorrect at index: 27704\n",
      "incorrect at index: 28516\n",
      "incorrect at index: 28518\n",
      "incorrect at index: 28696\n",
      "incorrect at index: 28785\n",
      "incorrect at index: 28802\n",
      "incorrect at index: 28935\n",
      "incorrect at index: 29029\n",
      "incorrect at index: 29359\n",
      "incorrect at index: 29444\n",
      "incorrect at index: 29616\n",
      "incorrect at index: 29775\n",
      "incorrect at index: 29935\n",
      "incorrect at index: 30008\n",
      "incorrect at index: 30120\n",
      "incorrect at index: 30169\n",
      "incorrect at index: 31388\n",
      "incorrect at index: 31792\n",
      "incorrect at index: 31813\n",
      "incorrect at index: 31929\n",
      "incorrect at index: 32154\n",
      "incorrect at index: 32609\n",
      "incorrect at index: 32844\n",
      "incorrect at index: 32944\n",
      "incorrect at index: 33049\n",
      "incorrect at index: 33071\n",
      "incorrect at index: 33181\n",
      "incorrect at index: 33204\n",
      "incorrect at index: 33908\n",
      "incorrect at index: 34014\n",
      "incorrect at index: 34035\n",
      "incorrect at index: 34095\n",
      "incorrect at index: 34315\n",
      "incorrect at index: 34385\n",
      "incorrect at index: 34431\n",
      "incorrect at index: 34553\n",
      "incorrect at index: 34654\n",
      "incorrect at index: 34764\n",
      "incorrect at index: 34814\n",
      "incorrect at index: 34829\n",
      "incorrect at index: 34852\n",
      "incorrect at index: 34877\n",
      "incorrect at index: 35056\n",
      "incorrect at index: 35121\n",
      "incorrect at index: 35542\n",
      "incorrect at index: 35687\n",
      "incorrect at index: 35763\n",
      "incorrect at index: 35860\n",
      "incorrect at index: 35926\n",
      "incorrect at index: 36288\n",
      "incorrect at index: 36777\n",
      "incorrect at index: 37607\n",
      "incorrect at index: 37788\n",
      "incorrect at index: 37869\n",
      "incorrect at index: 37986\n",
      "incorrect at index: 39110\n",
      "incorrect at index: 39113\n",
      "incorrect at index: 39226\n",
      "incorrect at index: 39242\n",
      "incorrect at index: 39274\n",
      "incorrect at index: 39648\n",
      "incorrect at index: 39758\n",
      "incorrect at index: 39896\n",
      "incorrect at index: 40225\n",
      "incorrect at index: 40574\n",
      "incorrect at index: 41029\n",
      "incorrect at index: 41329\n",
      "incorrect at index: 41798\n",
      "incorrect at index: 41887\n",
      "incorrect at index: 41954\n",
      "incorrect at index: 42009\n",
      "incorrect at index: 42555\n",
      "incorrect at index: 42678\n",
      "incorrect at index: 43243\n",
      "incorrect at index: 43376\n",
      "incorrect at index: 44018\n",
      "incorrect at index: 44518\n",
      "incorrect at index: 44580\n",
      "incorrect at index: 44650\n",
      "incorrect at index: 45004\n",
      "incorrect at index: 45382\n",
      "incorrect at index: 45722\n",
      "incorrect at index: 45825\n",
      "incorrect at index: 46541\n",
      "incorrect at index: 46543\n",
      "incorrect at index: 46663\n",
      "incorrect at index: 46759\n",
      "incorrect at index: 46979\n",
      "incorrect at index: 47827\n",
      "incorrect at index: 47846\n",
      "incorrect at index: 47884\n",
      "incorrect at index: 48964\n",
      "incorrect at index: 49313\n",
      "incorrect at index: 49315\n",
      "incorrect at index: 49406\n",
      "incorrect at index: 49586\n",
      "incorrect at index: 49679\n",
      "incorrect at index: 49930\n",
      "incorrect at index: 50272\n",
      "incorrect at index: 50383\n",
      "incorrect at index: 50715\n",
      "incorrect at index: 51103\n",
      "incorrect at index: 51106\n",
      "incorrect at index: 51499\n",
      "incorrect at index: 52030\n",
      "incorrect at index: 52290\n",
      "incorrect at index: 52630\n",
      "incorrect at index: 52762\n",
      "incorrect at index: 53061\n",
      "incorrect at index: 53306\n",
      "incorrect at index: 53779\n",
      "incorrect at index: 54031\n",
      "incorrect at index: 54206\n",
      "incorrect at index: 54314\n",
      "incorrect at index: 54356\n",
      "incorrect at index: 54589\n",
      "incorrect at index: 54834\n",
      "incorrect at index: 55379\n",
      "incorrect at index: 55470\n",
      "incorrect at index: 55693\n",
      "incorrect at index: 55849\n",
      "total percentage incorrect: 0.40714285714285714 %\n"
     ]
    }
   ],
   "source": [
    "wrong =0 \n",
    "wrong_arr = []\n",
    "wrong_arr_index = []\n",
    "correct = []\n",
    "\n",
    "for i, sample in enumerate(train_dataset):\n",
    "    snips, n_dig = find_digits(sample, 25)\n",
    "    real_num_digits = 5-list(train_labels[i]).count(10)\n",
    "\n",
    "    if(n_dig!=real_num_digits):\n",
    "        print(\"incorrect at index:\", i)\n",
    "        wrong+=1\n",
    "        wrong_arr.append(snips)\n",
    "        wrong_arr_index.append(i)  \n",
    "\n",
    "print(\"total percentage incorrect:\", wrong/len(train_dataset)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deprecated\n",
    "def create_batches(input_array, label_array, batch_size):\n",
    "    batched = []\n",
    "    label_batched = []\n",
    "    for i in range(np.floor(len(input_array)/batch_size).astype(int)):\n",
    "        batched.append(np.expand_dims((np.array(input_array[i*batch_size:i*batch_size+batch_size])/255*2-1).astype(np.single),axis=1))\n",
    "        label_batched.append(label_array[i*batch_size:i*batch_size+batch_size])\n",
    "    return np.array(batched), label_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainloader = torch.utils.data.DataLoader(xTrainingSet, batch_size=32,\n",
    "#                                          shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "training_set, training_set_labels = create_batches(xTrainingSet,yTrainingSet, batch_size)\n",
    "\n",
    "validation_set, validation_set_labels = create_batches(xValidationSet,yValidationSet, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, h_layers):\n",
    "        self.num_layers = len(h_layers)\n",
    "        self.h_layers = h_layers      \n",
    "        self.convs = []\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        for i, n in enumerate(self.h_layers):\n",
    "            if i == 0:\n",
    "                self.convs.append(nn.Conv2d(1, n, 3))\n",
    "            else:\n",
    "                print(h_layers[i-1],n)\n",
    "                self.convs.append(nn.Conv2d(self.h_layers[i-1], n, 3))\n",
    "\n",
    "        self.fc1 = nn.Linear(self.h_layers[-1]**2 , 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        for i in range(len(self.h_layers)):\n",
    "            x = F.relu(self.convs[i](x))\n",
    "        x = x.view(-1, self.h_layers[-1]**2)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "h_layers = ([batch_size,64])\n",
    "net = Net(h_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(training_set):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = torch.from_numpy(data)\n",
    "        labels = training_set_labels[i] \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        #print(inputs.shape,labels)\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(torch.from_numpy(np.array(labels).astype(np.longlong)))\n",
    "        loss = criterion(outputs, torch.from_numpy(np.array(labels).astype(np.longlong)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(validation_set):\n",
    "        images = torch.from_numpy(data)\n",
    "        labels = validation_set_labels[i]\n",
    "        labels = torch.from_numpy(np.array(labels).astype(np.longlong))\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if (int(predicted[0]) != int(labels[0])):\n",
    "            None\n",
    "            #displayGreyImage(np.squeeze(images)[0],f\"Predicted: {predicted[0]}, Target: {labels[0]}\")\n",
    "print(f\"Accuracy of the network on the {total} test images: {(100 * correct / total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
