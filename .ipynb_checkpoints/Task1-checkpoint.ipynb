{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBGRImage(path):\n",
    "    image = BGR(cv.imread(path))\n",
    "    return image\n",
    "\n",
    "def loadGreyImage(path):\n",
    "    image = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
    "    return image\n",
    "\n",
    "def BGR(image):\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "def displayGreyImage(image, imageName):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "\n",
    "def displayGreyWindows(image, imageName):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def displayBGRImage(image, imageName, size):\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(image)\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def displayBGRImageLarge(image, imageName):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    plt.imshow(image)\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def imageSideBySide(images, imageNames,size):\n",
    "    row = np.ceil(len(images)/20)\n",
    "    fig=plt.figure(figsize=(size, size/2))\n",
    "    for i, image in enumerate(images):\n",
    "        fig.add_subplot(row, 20, i+1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(imageNames[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_dataset', 'train_dataset', 'train_labels']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 25, 26\n",
    "size = 14, 12\n",
    "\n",
    "f = h5py.File('MNIST_synthetic.h5', 'r')\n",
    "\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.squeeze(np.array(f[\"train_dataset\"])).copy()\n",
    "train_labels = np.squeeze(np.array(f[\"train_labels\"])).copy()\n",
    "test_dataset = np.squeeze(np.array(f[\"test_dataset\"])).copy()\n",
    "\n",
    "singulars_digits = []\n",
    "singulars_labels = []\n",
    "\n",
    "doubles_digits = []\n",
    "doubles_labels = []\n",
    "\n",
    "triples_digits = []\n",
    "triples_labels = []\n",
    "\n",
    "quadruples_digits = []\n",
    "quadruples_labels = []\n",
    "\n",
    "quintuples_digits = []\n",
    "quintuples_labels = []\n",
    "\n",
    "\n",
    "for i, labels in enumerate(train_labels):\n",
    "    if labels[1] == 10:\n",
    "        singulars_digits.append(train_dataset[i])\n",
    "        singulars_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[1] != 10 and labels[2] == 10:\n",
    "        doubles_digits.append(train_dataset[i])\n",
    "        doubles_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[2] != 10 and labels[3] == 10:\n",
    "        triples_digits.append(train_dataset[i])\n",
    "        triples_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[3] != 10 and labels[4] == 10:\n",
    "        quadruples_digits.append(train_dataset[i])\n",
    "        quadruples_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[4] != 10:\n",
    "        quintuples_digits.append(train_dataset[i])\n",
    "        quintuples_labels.append(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "singulars_digits = np.array(singulars_digits)     \n",
    "doubles_digits = np.array(doubles_digits)  \n",
    "triples_digits = np.array(triples_digits)    \n",
    "quadruples_digits = np.array(quadruples_digits)    \n",
    "quintuples_digits = np.array(quintuples_digits)    \n",
    "\n",
    "singulars_labels = np.array(singulars_labels).T[0]\n",
    "doubles_labels = np.array(doubles_labels).T[0:2].T\n",
    "triples_label = np.array(triples_labels).T[0:3].T\n",
    "quadruples_label = np.array(quadruples_labels).T[0:4].T\n",
    "quintuples_label = np.array(quintuples_labels).T[0:5].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_singulars_dataset = []\n",
    "final_singulars_labels = list(singulars_labels)\n",
    "for i, digit in enumerate(singulars_digits):\n",
    "    final_singulars_dataset.append(digit[26:38,26:38])\n",
    "    \n",
    "final_doubles_dataset = []\n",
    "final_doubles_labels = []\n",
    "for i, digit in enumerate(doubles_digits):\n",
    "    final_doubles_dataset.append(digit[26:38,20:32])\n",
    "    final_doubles_dataset.append(digit[26:38,32:44])\n",
    "    final_doubles_labels.append(doubles_labels[i][0])\n",
    "    final_doubles_labels.append(doubles_labels[i][1])\n",
    "\n",
    "final_triples_dataset = []\n",
    "final_triples_labels = []\n",
    "for i, digit in enumerate(triples_digits):\n",
    "    final_triples_dataset.append(digit[26:38,14:26])\n",
    "    final_triples_dataset.append(digit[26:38,26:38])\n",
    "    final_triples_dataset.append(digit[26:38,38:50])\n",
    "    final_triples_labels.append(triples_labels[i][0])\n",
    "    final_triples_labels.append(triples_labels[i][1])\n",
    "    final_triples_labels.append(triples_labels[i][2])\n",
    "    \n",
    "final_quadruples_dataset = []\n",
    "final_quadruples_labels = []\n",
    "for i, digit in enumerate(quadruples_digits):\n",
    "    final_quadruples_dataset.append(digit[26:38,8:20])\n",
    "    final_quadruples_dataset.append(digit[26:38,20:32])\n",
    "    final_quadruples_dataset.append(digit[26:38,32:44])\n",
    "    final_quadruples_dataset.append(digit[26:38,44:56])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][0])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][1])    \n",
    "    final_quadruples_labels.append(quadruples_labels[i][2])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][3]) \n",
    "    \n",
    "final_quintuples_dataset = []\n",
    "final_quintuples_labels = []\n",
    "for i, digit in enumerate(quintuples_digits):\n",
    "    final_quintuples_dataset.append(digit[26:38,2:14])\n",
    "    final_quintuples_dataset.append(digit[26:38,14:26])\n",
    "    final_quintuples_dataset.append(digit[26:38,26:38])\n",
    "    final_quintuples_dataset.append(digit[26:38,38:50])\n",
    "    final_quintuples_dataset.append(digit[26:38,50:62])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][0])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][1])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][2])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][3])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = final_singulars_dataset + final_doubles_dataset + final_triples_dataset + final_quadruples_dataset + final_quintuples_dataset\n",
    "merged_labels = final_singulars_labels + final_doubles_labels + final_triples_labels + final_quadruples_labels + final_quintuples_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 0's in train dataset: 16379\n",
      "number of 1's in train dataset: 18819\n",
      "number of 2's in train dataset: 17095\n",
      "number of 3's in train dataset: 17220\n",
      "number of 4's in train dataset: 16393\n",
      "number of 5's in train dataset: 15275\n",
      "number of 6's in train dataset: 16401\n",
      "number of 7's in train dataset: 17611\n",
      "number of 8's in train dataset: 16175\n",
      "number of 9's in train dataset: 16518\n",
      "total number of digits in train dataset: 167886\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for digit in range(10):\n",
    "    digit_count = list(merged_labels).count(digit)\n",
    "    print(f\"number of {digit}'s in train dataset: {digit_count}\")\n",
    "    count += digit_count\n",
    "\n",
    "print(\"total number of digits in train dataset:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(merged_dataset)))\n",
    "digitsValidationSetSize = int(len(merged_labels) - digitsTrainingSetSize)\n",
    "\n",
    "xValidationSet = []\n",
    "yValidationSet = []\n",
    "\n",
    "for index, digit in enumerate(merged_dataset[0:digitsValidationSetSize]):\n",
    "    xValidationSet.append(digit)\n",
    "    yValidationSet.append(merged_labels[index])\n",
    "\n",
    "xTrainingSet = []\n",
    "yTrainingSet = []\n",
    "\n",
    "start = len(xValidationSet)\n",
    "\n",
    "for i, digit in enumerate(merged_dataset[start:]):\n",
    "    xTrainingSet.append(digit)\n",
    "    yTrainingSet.append(merged_labels[i+start])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(input_array, label_array, batch_size):\n",
    "    batched = []\n",
    "    label_batched = []\n",
    "    \n",
    "    for i in range(np.floor(len(input_array)/batch_size).astype(int)):\n",
    "        batched.append(np.expand_dims((np.array(input_array[i*batch_size:i*batch_size+batch_size])/255*2-1).astype(np.single),axis=1))\n",
    "        label_batched.append(label_array[i*batch_size:i*batch_size+batch_size])\n",
    "        \n",
    "    return np.array(batched), label_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, h_layers):\n",
    "        self.num_layers = len(h_layers)\n",
    "        self.h_layers = h_layers      \n",
    "        self.convs = []\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        for i, n in enumerate(self.h_layers):\n",
    "            if i == 0:\n",
    "                self.convs.append(nn.Conv2d(1, n, 3))\n",
    "            else:\n",
    "                self.convs.append(nn.Conv2d(self.h_layers[i-1], n, 3))\n",
    "\n",
    "        self.fc1 = nn.Linear(self.h_layers[-1]**2 , 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.h_layers)):\n",
    "            x = F.relu(self.convs[i](x))\n",
    "            \n",
    "        x = x.view(-1, self.h_layers[-1]**2)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.194\n",
      "[1,   400] loss: 0.140\n",
      "[1,   600] loss: 0.111\n",
      "[1,   800] loss: 0.096\n",
      "[1,  1000] loss: 0.082\n",
      "[1,  1200] loss: 0.077\n",
      "[1,  1400] loss: 0.072\n",
      "[1,  1600] loss: 0.066\n",
      "[1,  1800] loss: 0.064\n",
      "[1,  2000] loss: 0.061\n",
      "[1,  2200] loss: 0.060\n",
      "[1,  2400] loss: 0.058\n",
      "[1,  2600] loss: 0.054\n",
      "[1,  2800] loss: 0.052\n",
      "[1,  3000] loss: 0.052\n",
      "[1,  3200] loss: 0.051\n",
      "[1,  3400] loss: 0.049\n",
      "[1,  3600] loss: 0.048\n",
      "[1,  3800] loss: 0.047\n",
      "[1,  4000] loss: 0.045\n",
      "[1,  4200] loss: 0.044\n",
      "[1,  4400] loss: 0.044\n",
      "[1,  4600] loss: 0.044\n",
      "[1,  4800] loss: 0.043\n",
      "[1,  5000] loss: 0.044\n",
      "[1,  5200] loss: 0.045\n",
      "[2,   200] loss: 0.042\n",
      "[2,   400] loss: 0.043\n",
      "[2,   600] loss: 0.040\n",
      "[2,   800] loss: 0.041\n",
      "[2,  1000] loss: 0.037\n",
      "[2,  1200] loss: 0.040\n",
      "[2,  1400] loss: 0.039\n",
      "[2,  1600] loss: 0.038\n",
      "[2,  1800] loss: 0.038\n",
      "[2,  2000] loss: 0.038\n",
      "[2,  2200] loss: 0.039\n",
      "[2,  2400] loss: 0.039\n",
      "[2,  2600] loss: 0.036\n",
      "[2,  2800] loss: 0.035\n",
      "[2,  3000] loss: 0.037\n",
      "[2,  3200] loss: 0.037\n",
      "[2,  3400] loss: 0.036\n",
      "[2,  3600] loss: 0.036\n",
      "[2,  3800] loss: 0.034\n",
      "[2,  4000] loss: 0.034\n",
      "[2,  4200] loss: 0.034\n",
      "[2,  4400] loss: 0.034\n",
      "[2,  4600] loss: 0.034\n",
      "[2,  4800] loss: 0.034\n",
      "[2,  5000] loss: 0.035\n",
      "[2,  5200] loss: 0.036\n",
      "[3,   200] loss: 0.034\n",
      "[3,   400] loss: 0.035\n",
      "[3,   600] loss: 0.033\n",
      "[3,   800] loss: 0.034\n",
      "[3,  1000] loss: 0.031\n",
      "[3,  1200] loss: 0.033\n",
      "[3,  1400] loss: 0.033\n",
      "[3,  1600] loss: 0.032\n",
      "[3,  1800] loss: 0.032\n",
      "[3,  2000] loss: 0.033\n",
      "[3,  2200] loss: 0.034\n",
      "[3,  2400] loss: 0.034\n",
      "[3,  2600] loss: 0.031\n",
      "[3,  2800] loss: 0.030\n",
      "[3,  3000] loss: 0.032\n",
      "[3,  3200] loss: 0.032\n",
      "[3,  3400] loss: 0.031\n",
      "[3,  3600] loss: 0.032\n",
      "[3,  3800] loss: 0.030\n",
      "[3,  4000] loss: 0.029\n",
      "[3,  4200] loss: 0.029\n",
      "[3,  4400] loss: 0.030\n",
      "[3,  4600] loss: 0.030\n",
      "[3,  4800] loss: 0.030\n",
      "[3,  5000] loss: 0.031\n",
      "[3,  5200] loss: 0.033\n",
      "[4,   200] loss: 0.030\n",
      "[4,   400] loss: 0.031\n",
      "[4,   600] loss: 0.030\n",
      "[4,   800] loss: 0.031\n",
      "[4,  1000] loss: 0.027\n",
      "[4,  1200] loss: 0.030\n",
      "[4,  1400] loss: 0.030\n",
      "[4,  1600] loss: 0.029\n",
      "[4,  1800] loss: 0.029\n",
      "[4,  2000] loss: 0.030\n",
      "[4,  2200] loss: 0.031\n",
      "[4,  2400] loss: 0.031\n",
      "[4,  2600] loss: 0.028\n",
      "[4,  2800] loss: 0.028\n",
      "[4,  3000] loss: 0.029\n",
      "[4,  3200] loss: 0.029\n",
      "[4,  3400] loss: 0.028\n",
      "[4,  3600] loss: 0.029\n",
      "[4,  3800] loss: 0.027\n",
      "[4,  4000] loss: 0.027\n",
      "[4,  4200] loss: 0.027\n",
      "[4,  4400] loss: 0.027\n",
      "[4,  4600] loss: 0.027\n",
      "[4,  4800] loss: 0.027\n",
      "[4,  5000] loss: 0.029\n",
      "[4,  5200] loss: 0.030\n",
      "[5,   200] loss: 0.028\n",
      "[5,   400] loss: 0.029\n",
      "[5,   600] loss: 0.027\n",
      "[5,   800] loss: 0.028\n",
      "[5,  1000] loss: 0.025\n",
      "[5,  1200] loss: 0.028\n",
      "[5,  1400] loss: 0.027\n",
      "[5,  1600] loss: 0.027\n",
      "[5,  1800] loss: 0.027\n",
      "[5,  2000] loss: 0.028\n",
      "[5,  2200] loss: 0.029\n",
      "[5,  2400] loss: 0.029\n",
      "[5,  2600] loss: 0.026\n",
      "[5,  2800] loss: 0.026\n",
      "[5,  3000] loss: 0.027\n",
      "[5,  3200] loss: 0.027\n",
      "[5,  3400] loss: 0.027\n",
      "[5,  3600] loss: 0.027\n",
      "[5,  3800] loss: 0.025\n",
      "[5,  4000] loss: 0.025\n",
      "[5,  4200] loss: 0.025\n",
      "[5,  4400] loss: 0.026\n",
      "[5,  4600] loss: 0.026\n",
      "[5,  4800] loss: 0.025\n",
      "[5,  5000] loss: 0.027\n",
      "[5,  5200] loss: 0.029\n",
      "[6,   200] loss: 0.026\n",
      "[6,   400] loss: 0.027\n",
      "[6,   600] loss: 0.026\n",
      "[6,   800] loss: 0.027\n",
      "[6,  1000] loss: 0.024\n",
      "[6,  1200] loss: 0.026\n",
      "[6,  1400] loss: 0.026\n",
      "[6,  1600] loss: 0.025\n",
      "[6,  1800] loss: 0.026\n",
      "[6,  2000] loss: 0.026\n",
      "[6,  2200] loss: 0.028\n",
      "[6,  2400] loss: 0.028\n",
      "[6,  2600] loss: 0.025\n",
      "[6,  2800] loss: 0.024\n",
      "[6,  3000] loss: 0.025\n",
      "[6,  3200] loss: 0.025\n",
      "[6,  3400] loss: 0.025\n",
      "[6,  3600] loss: 0.026\n",
      "[6,  3800] loss: 0.024\n",
      "[6,  4000] loss: 0.024\n",
      "[6,  4200] loss: 0.024\n",
      "[6,  4400] loss: 0.024\n",
      "[6,  4600] loss: 0.024\n",
      "[6,  4800] loss: 0.024\n",
      "[6,  5000] loss: 0.026\n",
      "[6,  5200] loss: 0.027\n",
      "[7,   200] loss: 0.024\n",
      "[7,   400] loss: 0.026\n",
      "[7,   600] loss: 0.024\n",
      "[7,   800] loss: 0.025\n",
      "[7,  1000] loss: 0.023\n",
      "[7,  1200] loss: 0.025\n",
      "[7,  1400] loss: 0.024\n",
      "[7,  1600] loss: 0.024\n",
      "[7,  1800] loss: 0.024\n",
      "[7,  2000] loss: 0.025\n",
      "[7,  2200] loss: 0.027\n",
      "[7,  2400] loss: 0.026\n",
      "[7,  2600] loss: 0.024\n",
      "[7,  2800] loss: 0.023\n",
      "[7,  3000] loss: 0.024\n",
      "[7,  3200] loss: 0.024\n",
      "[7,  3400] loss: 0.024\n",
      "[7,  3600] loss: 0.025\n",
      "[7,  3800] loss: 0.023\n",
      "[7,  4000] loss: 0.022\n",
      "[7,  4200] loss: 0.023\n",
      "[7,  4400] loss: 0.023\n",
      "[7,  4600] loss: 0.023\n",
      "[7,  4800] loss: 0.023\n",
      "[7,  5000] loss: 0.024\n",
      "[7,  5200] loss: 0.026\n",
      "[8,   200] loss: 0.023\n",
      "[8,   400] loss: 0.025\n",
      "[8,   600] loss: 0.023\n",
      "[8,   800] loss: 0.024\n",
      "[8,  1000] loss: 0.022\n",
      "[8,  1200] loss: 0.024\n",
      "[8,  1400] loss: 0.023\n",
      "[8,  1600] loss: 0.023\n",
      "[8,  1800] loss: 0.023\n",
      "[8,  2000] loss: 0.024\n",
      "[8,  2200] loss: 0.026\n",
      "[8,  2400] loss: 0.025\n",
      "[8,  2600] loss: 0.023\n",
      "[8,  2800] loss: 0.022\n",
      "[8,  3000] loss: 0.023\n",
      "[8,  3200] loss: 0.023\n",
      "[8,  3400] loss: 0.023\n",
      "[8,  3600] loss: 0.024\n",
      "[8,  3800] loss: 0.022\n",
      "[8,  4000] loss: 0.022\n",
      "[8,  4200] loss: 0.022\n",
      "[8,  4400] loss: 0.022\n",
      "[8,  4600] loss: 0.022\n",
      "[8,  4800] loss: 0.022\n",
      "[8,  5000] loss: 0.023\n",
      "[8,  5200] loss: 0.025\n",
      "[9,   200] loss: 0.022\n",
      "[9,   400] loss: 0.024\n",
      "[9,   600] loss: 0.023\n",
      "[9,   800] loss: 0.024\n",
      "[9,  1000] loss: 0.021\n",
      "[9,  1200] loss: 0.023\n",
      "[9,  1400] loss: 0.022\n",
      "[9,  1600] loss: 0.023\n",
      "[9,  1800] loss: 0.023\n",
      "[9,  2000] loss: 0.023\n",
      "[9,  2200] loss: 0.025\n",
      "[9,  2400] loss: 0.024\n",
      "[9,  2600] loss: 0.022\n",
      "[9,  2800] loss: 0.021\n",
      "[9,  3000] loss: 0.022\n",
      "[9,  3200] loss: 0.022\n",
      "[9,  3400] loss: 0.022\n",
      "[9,  3600] loss: 0.023\n",
      "[9,  3800] loss: 0.021\n",
      "[9,  4000] loss: 0.021\n",
      "[9,  4200] loss: 0.021\n",
      "[9,  4400] loss: 0.022\n",
      "[9,  4600] loss: 0.022\n",
      "[9,  4800] loss: 0.021\n",
      "[9,  5000] loss: 0.023\n",
      "[9,  5200] loss: 0.025\n",
      "[10,   200] loss: 0.021\n",
      "[10,   400] loss: 0.023\n",
      "[10,   600] loss: 0.022\n",
      "[10,   800] loss: 0.023\n",
      "[10,  1000] loss: 0.020\n",
      "[10,  1200] loss: 0.022\n",
      "[10,  1400] loss: 0.022\n",
      "[10,  1600] loss: 0.022\n",
      "[10,  1800] loss: 0.022\n",
      "[10,  2000] loss: 0.022\n",
      "[10,  2200] loss: 0.024\n",
      "[10,  2400] loss: 0.024\n",
      "[10,  2600] loss: 0.021\n",
      "[10,  2800] loss: 0.021\n",
      "[10,  3000] loss: 0.022\n",
      "[10,  3200] loss: 0.021\n",
      "[10,  3400] loss: 0.022\n",
      "[10,  3600] loss: 0.023\n",
      "[10,  3800] loss: 0.020\n",
      "[10,  4000] loss: 0.020\n",
      "[10,  4200] loss: 0.020\n",
      "[10,  4400] loss: 0.021\n",
      "[10,  4600] loss: 0.021\n",
      "[10,  4800] loss: 0.021\n",
      "[10,  5000] loss: 0.022\n",
      "[10,  5200] loss: 0.024\n",
      "validation set accuracy (33575 samples): 94.12062546537602\n"
     ]
    }
   ],
   "source": [
    "# finding the best hyper-parameters for the neural network\n",
    "\n",
    "batch_size = 25\n",
    "training_set, training_set_labels = create_batches(xTrainingSet, yTrainingSet, batch_size)\n",
    "validation_set, validation_set_labels = create_batches(xValidationSet, yValidationSet, batch_size)\n",
    "\n",
    "# model hyper-parameters\n",
    "h_layers = ([batch_size, 64])\n",
    "max_iters = 10\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "# train neural network\n",
    "net = Net(h_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "for epoch in range(max_iters):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(training_set):\n",
    "        inputs = torch.from_numpy(data)\n",
    "        labels = training_set_labels[i] \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.from_numpy(np.array(labels).astype(np.longlong)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print every 200 mini-batches\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "# test validation set on model\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(validation_set):\n",
    "        images = torch.from_numpy(data)\n",
    "        labels = validation_set_labels[i]\n",
    "        labels = torch.from_numpy(np.array(labels).astype(np.longlong))\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if (int(predicted[0]) != int(labels[0])):\n",
    "            None\n",
    "\n",
    "print(f\"validation set accuracy ({total} samples): {(100 * correct / total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_digits(train_dataset, size):\n",
    "    thresh_train_dataset = train_dataset.copy()\n",
    "    thresh_train_dataset[thresh_train_dataset>30] = 255\n",
    "    thresh_train_dataset[thresh_train_dataset!=255] = 0\n",
    "    i,j = np.where(thresh_train_dataset[:,:]!=0)\n",
    "    #print(i)\n",
    "    bottom_bound = np.min(i)\n",
    "    top_bound = np.max(i)\n",
    "    left_bound = np.min(j)\n",
    "    right_bound = np.max(j)\n",
    "    #print(top_bound, bottom_bound)\n",
    "\n",
    "    h = top_bound-bottom_bound\n",
    "    w = right_bound-left_bound\n",
    "\n",
    "    bounded_train_dataset = train_dataset[bottom_bound-1:top_bound+2,left_bound-1:right_bound+2]\n",
    "    thresh = bounded_train_dataset.copy()\n",
    "    thresh[thresh>30] = 255\n",
    "    thresh[thresh!=255] = 0\n",
    "    #ret, thresh = cv2.threshold(bounded_train_dataset, 30, 255, 0)\n",
    "    #seg = np.where(np.any(thresh, axis=0)==0)\n",
    "    seg = np.where(np.any(thresh, axis=0)==0)\n",
    "\n",
    "    #print(seg)\n",
    "    seg_list = np.asarray(seg)\n",
    "    seg_list = seg_list[0]\n",
    "    #print(seg_list)\n",
    "    from statistics import stdev\n",
    "\n",
    "    if len(seg_list)>2:\n",
    "        # create a list of the gaps between the consecutive values\n",
    "        gaps = [y - x for x, y in zip(seg_list[:-1], seg_list[1:])]\n",
    "        # have python calculate the standard deviation for the gaps\n",
    "        sd = stdev(gaps)\n",
    "\n",
    "        # create a list of lists, put the first value of the source data in the first\n",
    "        lists = [[seg_list[0]]]\n",
    "        for x in seg_list[1:]:\n",
    "            # if the gap from the current item to the previous is more than 1 SD\n",
    "            # Note: the previous item is the last item in the last list\n",
    "            # Note: the '> 1' is the part you'd modify to make it stricter or more relaxed\n",
    "            if (x - lists[-1][-1]) / (sd+1e-18) > 0.8:\n",
    "                # then start a new list\n",
    "                lists.append([])\n",
    "            # add the current item to the last list in the list\n",
    "            lists[-1].append(x)\n",
    "\n",
    "        splits = np.asarray([np.ceil(np.mean(lists[i])) for i in range(len(lists))]).astype(int)\n",
    "\n",
    "    else:\n",
    "        splits = np.asarray(seg_list)\n",
    "    #print(splits)\n",
    "\n",
    "    n_digits = len(splits)-1\n",
    "    #digits = np.zeros(n_digits, )\n",
    "    digits = []\n",
    "    for i in range(n_digits):\n",
    "        temp = bounded_train_dataset[:,splits[i]:splits[i+1]]\n",
    "        # if temp is less than recommeded size first pad on left and then on both sides\n",
    "        temp_padded = temp.copy()\n",
    "        if (temp.shape[0] != size) or  (temp.shape[1] != size):\n",
    "            diff_y = size-temp.shape[0]\n",
    "            split_diff_y = diff_y//2\n",
    "            remainder_diff_y = diff_y%2\n",
    "            diff_x = size-temp.shape[1]\n",
    "            split_diff_x = diff_x//2\n",
    "            remainder_diff_x = diff_x%2\n",
    "            #print(diff_y)\n",
    "            #print(split_diff_y)\n",
    "            temp_padded = np.pad(temp, ((split_diff_y+remainder_diff_y,split_diff_y),(split_diff_x+remainder_diff_x,split_diff_x)))\n",
    "\n",
    "        digits.append(temp_padded)\n",
    "    return digits, n_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect at index: 367\n",
      "incorrect at index: 1351\n",
      "incorrect at index: 1606\n",
      "incorrect at index: 2684\n",
      "incorrect at index: 3172\n",
      "incorrect at index: 3771\n",
      "incorrect at index: 3838\n",
      "incorrect at index: 3883\n",
      "incorrect at index: 4011\n",
      "incorrect at index: 4447\n",
      "incorrect at index: 4639\n",
      "incorrect at index: 4747\n",
      "incorrect at index: 5419\n",
      "incorrect at index: 5442\n",
      "incorrect at index: 5772\n",
      "incorrect at index: 5930\n",
      "incorrect at index: 7118\n",
      "incorrect at index: 7981\n",
      "incorrect at index: 8652\n",
      "incorrect at index: 8860\n",
      "incorrect at index: 8941\n",
      "incorrect at index: 9038\n",
      "incorrect at index: 9176\n",
      "incorrect at index: 9419\n",
      "incorrect at index: 10702\n",
      "incorrect at index: 11622\n",
      "incorrect at index: 11810\n",
      "incorrect at index: 12806\n",
      "incorrect at index: 13280\n",
      "incorrect at index: 13360\n",
      "incorrect at index: 13514\n",
      "incorrect at index: 14150\n",
      "incorrect at index: 14931\n",
      "incorrect at index: 15844\n",
      "incorrect at index: 16366\n",
      "incorrect at index: 16909\n",
      "incorrect at index: 17030\n",
      "incorrect at index: 17540\n",
      "incorrect at index: 17573\n",
      "incorrect at index: 18122\n",
      "incorrect at index: 18127\n",
      "incorrect at index: 18460\n",
      "incorrect at index: 19104\n",
      "incorrect at index: 19660\n",
      "incorrect at index: 20029\n",
      "incorrect at index: 20764\n",
      "incorrect at index: 21153\n",
      "incorrect at index: 21854\n",
      "incorrect at index: 21980\n",
      "incorrect at index: 22025\n",
      "incorrect at index: 22369\n",
      "incorrect at index: 22596\n",
      "incorrect at index: 23044\n",
      "incorrect at index: 23684\n",
      "incorrect at index: 24026\n",
      "incorrect at index: 24518\n",
      "incorrect at index: 25190\n",
      "incorrect at index: 25233\n",
      "incorrect at index: 25273\n",
      "incorrect at index: 26042\n",
      "incorrect at index: 26094\n",
      "incorrect at index: 26601\n",
      "incorrect at index: 27704\n",
      "incorrect at index: 28516\n",
      "incorrect at index: 28518\n",
      "incorrect at index: 28802\n",
      "incorrect at index: 29359\n",
      "incorrect at index: 29444\n",
      "incorrect at index: 29616\n",
      "incorrect at index: 29775\n",
      "incorrect at index: 30169\n",
      "incorrect at index: 31388\n",
      "incorrect at index: 32154\n",
      "incorrect at index: 32609\n",
      "incorrect at index: 33049\n",
      "incorrect at index: 33071\n",
      "incorrect at index: 33204\n",
      "incorrect at index: 33908\n",
      "incorrect at index: 34014\n",
      "incorrect at index: 34814\n",
      "incorrect at index: 34852\n",
      "incorrect at index: 34877\n",
      "incorrect at index: 35542\n",
      "incorrect at index: 36288\n",
      "incorrect at index: 36777\n",
      "incorrect at index: 37607\n",
      "incorrect at index: 37788\n",
      "incorrect at index: 37869\n",
      "incorrect at index: 37986\n",
      "incorrect at index: 39113\n",
      "incorrect at index: 39226\n",
      "incorrect at index: 39274\n",
      "incorrect at index: 39648\n",
      "incorrect at index: 39758\n",
      "incorrect at index: 39896\n",
      "incorrect at index: 40225\n",
      "incorrect at index: 40457\n",
      "incorrect at index: 41329\n",
      "incorrect at index: 41954\n",
      "incorrect at index: 42009\n",
      "incorrect at index: 42555\n",
      "incorrect at index: 42678\n",
      "incorrect at index: 44367\n",
      "incorrect at index: 44518\n",
      "incorrect at index: 44650\n",
      "incorrect at index: 45004\n",
      "incorrect at index: 45382\n",
      "incorrect at index: 46543\n",
      "incorrect at index: 46663\n",
      "incorrect at index: 46759\n",
      "incorrect at index: 46979\n",
      "incorrect at index: 47846\n",
      "incorrect at index: 47884\n",
      "incorrect at index: 48964\n",
      "incorrect at index: 49315\n",
      "incorrect at index: 49406\n",
      "incorrect at index: 50272\n",
      "incorrect at index: 50383\n",
      "incorrect at index: 50715\n",
      "incorrect at index: 50729\n",
      "incorrect at index: 51103\n",
      "incorrect at index: 51106\n",
      "incorrect at index: 52290\n",
      "incorrect at index: 52762\n",
      "incorrect at index: 53061\n",
      "incorrect at index: 54031\n",
      "incorrect at index: 54206\n",
      "incorrect at index: 54314\n",
      "incorrect at index: 54356\n",
      "incorrect at index: 54528\n",
      "incorrect at index: 55379\n",
      "incorrect at index: 55470\n",
      "incorrect at index: 55693\n",
      "incorrect at index: 55849\n",
      "incorrect at index: 55866\n",
      "total percentage incorrect: 0.24107142857142855 %\n"
     ]
    }
   ],
   "source": [
    "# testing finding individual digit images in the train dataset\n",
    "\n",
    "wrong = 0 \n",
    "wrong_arr = []\n",
    "wrong_arr_index = []\n",
    "correct = []\n",
    "\n",
    "for i, sample in enumerate(train_dataset):\n",
    "    snips, n_dig = find_digits(sample, 25)\n",
    "    real_num_digits = 5 - list(train_labels[i]).count(10)\n",
    "\n",
    "    if(n_dig != real_num_digits):\n",
    "        print(\"incorrect at index:\", i)\n",
    "        wrong += 1\n",
    "        wrong_arr.append(snips)\n",
    "        wrong_arr_index.append(i)  \n",
    "\n",
    "print(\"total percentage incorrect:\", wrong / len(train_dataset) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.196\n",
      "[1,   400] loss: 0.145\n",
      "[1,   600] loss: 0.116\n",
      "[1,   800] loss: 0.099\n",
      "[1,  1000] loss: 0.085\n",
      "[1,  1200] loss: 0.080\n",
      "[1,  1400] loss: 0.074\n",
      "[1,  1600] loss: 0.068\n",
      "[1,  1800] loss: 0.065\n",
      "[1,  2000] loss: 0.062\n",
      "[1,  2200] loss: 0.061\n",
      "[1,  2400] loss: 0.059\n",
      "[1,  2600] loss: 0.055\n",
      "[1,  2800] loss: 0.052\n",
      "[1,  3000] loss: 0.053\n",
      "[1,  3200] loss: 0.051\n",
      "[1,  3400] loss: 0.049\n",
      "[1,  3600] loss: 0.049\n",
      "[1,  3800] loss: 0.047\n",
      "[1,  4000] loss: 0.045\n",
      "[1,  4200] loss: 0.045\n",
      "[1,  4400] loss: 0.044\n",
      "[1,  4600] loss: 0.044\n",
      "[1,  4800] loss: 0.043\n",
      "[1,  5000] loss: 0.044\n",
      "[1,  5200] loss: 0.045\n",
      "[2,   200] loss: 0.042\n",
      "[2,   400] loss: 0.042\n",
      "[2,   600] loss: 0.040\n",
      "[2,   800] loss: 0.040\n",
      "[2,  1000] loss: 0.037\n",
      "[2,  1200] loss: 0.040\n",
      "[2,  1400] loss: 0.039\n",
      "[2,  1600] loss: 0.037\n",
      "[2,  1800] loss: 0.038\n",
      "[2,  2000] loss: 0.038\n",
      "[2,  2200] loss: 0.039\n",
      "[2,  2400] loss: 0.039\n",
      "[2,  2600] loss: 0.036\n",
      "[2,  2800] loss: 0.035\n",
      "[2,  3000] loss: 0.036\n",
      "[2,  3200] loss: 0.036\n",
      "[2,  3400] loss: 0.035\n",
      "[2,  3600] loss: 0.036\n",
      "[2,  3800] loss: 0.034\n",
      "[2,  4000] loss: 0.033\n",
      "[2,  4200] loss: 0.033\n",
      "[2,  4400] loss: 0.034\n",
      "[2,  4600] loss: 0.033\n",
      "[2,  4800] loss: 0.033\n",
      "[2,  5000] loss: 0.034\n",
      "[2,  5200] loss: 0.036\n",
      "[3,   200] loss: 0.033\n",
      "[3,   400] loss: 0.034\n",
      "[3,   600] loss: 0.032\n",
      "[3,   800] loss: 0.033\n",
      "[3,  1000] loss: 0.030\n",
      "[3,  1200] loss: 0.033\n",
      "[3,  1400] loss: 0.032\n",
      "[3,  1600] loss: 0.031\n",
      "[3,  1800] loss: 0.032\n",
      "[3,  2000] loss: 0.032\n",
      "[3,  2200] loss: 0.033\n",
      "[3,  2400] loss: 0.033\n",
      "[3,  2600] loss: 0.031\n",
      "[3,  2800] loss: 0.030\n",
      "[3,  3000] loss: 0.031\n",
      "[3,  3200] loss: 0.031\n",
      "[3,  3400] loss: 0.030\n",
      "[3,  3600] loss: 0.031\n",
      "[3,  3800] loss: 0.029\n",
      "[3,  4000] loss: 0.028\n",
      "[3,  4200] loss: 0.029\n",
      "[3,  4400] loss: 0.029\n",
      "[3,  4600] loss: 0.029\n",
      "[3,  4800] loss: 0.029\n",
      "[3,  5000] loss: 0.030\n",
      "[3,  5200] loss: 0.032\n",
      "[4,   200] loss: 0.029\n",
      "[4,   400] loss: 0.030\n",
      "[4,   600] loss: 0.029\n",
      "[4,   800] loss: 0.029\n",
      "[4,  1000] loss: 0.026\n",
      "[4,  1200] loss: 0.029\n",
      "[4,  1400] loss: 0.029\n",
      "[4,  1600] loss: 0.028\n",
      "[4,  1800] loss: 0.028\n",
      "[4,  2000] loss: 0.028\n",
      "[4,  2200] loss: 0.030\n",
      "[4,  2400] loss: 0.030\n",
      "[4,  2600] loss: 0.027\n",
      "[4,  2800] loss: 0.027\n",
      "[4,  3000] loss: 0.028\n",
      "[4,  3200] loss: 0.028\n",
      "[4,  3400] loss: 0.027\n",
      "[4,  3600] loss: 0.028\n",
      "[4,  3800] loss: 0.026\n",
      "[4,  4000] loss: 0.026\n",
      "[4,  4200] loss: 0.026\n",
      "[4,  4400] loss: 0.026\n",
      "[4,  4600] loss: 0.026\n",
      "[4,  4800] loss: 0.026\n",
      "[4,  5000] loss: 0.027\n",
      "[4,  5200] loss: 0.029\n",
      "[5,   200] loss: 0.027\n",
      "[5,   400] loss: 0.028\n",
      "[5,   600] loss: 0.026\n",
      "[5,   800] loss: 0.027\n",
      "[5,  1000] loss: 0.024\n",
      "[5,  1200] loss: 0.027\n",
      "[5,  1400] loss: 0.026\n",
      "[5,  1600] loss: 0.026\n",
      "[5,  1800] loss: 0.026\n",
      "[5,  2000] loss: 0.026\n",
      "[5,  2200] loss: 0.028\n",
      "[5,  2400] loss: 0.028\n",
      "[5,  2600] loss: 0.025\n",
      "[5,  2800] loss: 0.025\n",
      "[5,  3000] loss: 0.025\n",
      "[5,  3200] loss: 0.026\n",
      "[5,  3400] loss: 0.025\n",
      "[5,  3600] loss: 0.026\n",
      "[5,  3800] loss: 0.024\n",
      "[5,  4000] loss: 0.024\n",
      "[5,  4200] loss: 0.024\n",
      "[5,  4400] loss: 0.025\n",
      "[5,  4600] loss: 0.025\n",
      "[5,  4800] loss: 0.024\n",
      "[5,  5000] loss: 0.026\n",
      "[5,  5200] loss: 0.027\n",
      "[6,   200] loss: 0.025\n",
      "[6,   400] loss: 0.026\n",
      "[6,   600] loss: 0.025\n",
      "[6,   800] loss: 0.025\n",
      "[6,  1000] loss: 0.023\n",
      "[6,  1200] loss: 0.025\n",
      "[6,  1400] loss: 0.024\n",
      "[6,  1600] loss: 0.024\n",
      "[6,  1800] loss: 0.024\n",
      "[6,  2000] loss: 0.025\n",
      "[6,  2200] loss: 0.026\n",
      "[6,  2400] loss: 0.026\n",
      "[6,  2600] loss: 0.024\n",
      "[6,  2800] loss: 0.023\n",
      "[6,  3000] loss: 0.024\n",
      "[6,  3200] loss: 0.024\n",
      "[6,  3400] loss: 0.024\n",
      "[6,  3600] loss: 0.025\n",
      "[6,  3800] loss: 0.023\n",
      "[6,  4000] loss: 0.022\n",
      "[6,  4200] loss: 0.023\n",
      "[6,  4400] loss: 0.023\n",
      "[6,  4600] loss: 0.023\n",
      "[6,  4800] loss: 0.023\n",
      "[6,  5000] loss: 0.024\n",
      "[6,  5200] loss: 0.026\n",
      "[7,   200] loss: 0.023\n",
      "[7,   400] loss: 0.025\n",
      "[7,   600] loss: 0.023\n",
      "[7,   800] loss: 0.024\n",
      "[7,  1000] loss: 0.021\n",
      "[7,  1200] loss: 0.023\n",
      "[7,  1400] loss: 0.023\n",
      "[7,  1600] loss: 0.023\n",
      "[7,  1800] loss: 0.023\n",
      "[7,  2000] loss: 0.023\n",
      "[7,  2200] loss: 0.025\n",
      "[7,  2400] loss: 0.025\n",
      "[7,  2600] loss: 0.023\n",
      "[7,  2800] loss: 0.022\n",
      "[7,  3000] loss: 0.023\n",
      "[7,  3200] loss: 0.023\n",
      "[7,  3400] loss: 0.022\n",
      "[7,  3600] loss: 0.023\n",
      "[7,  3800] loss: 0.022\n",
      "[7,  4000] loss: 0.021\n",
      "[7,  4200] loss: 0.021\n",
      "[7,  4400] loss: 0.022\n",
      "[7,  4600] loss: 0.022\n",
      "[7,  4800] loss: 0.022\n",
      "[7,  5000] loss: 0.023\n",
      "[7,  5200] loss: 0.025\n",
      "[8,   200] loss: 0.022\n",
      "[8,   400] loss: 0.024\n",
      "[8,   600] loss: 0.022\n",
      "[8,   800] loss: 0.023\n",
      "[8,  1000] loss: 0.020\n",
      "[8,  1200] loss: 0.022\n",
      "[8,  1400] loss: 0.022\n",
      "[8,  1600] loss: 0.022\n",
      "[8,  1800] loss: 0.022\n",
      "[8,  2000] loss: 0.022\n",
      "[8,  2200] loss: 0.024\n",
      "[8,  2400] loss: 0.024\n",
      "[8,  2600] loss: 0.022\n",
      "[8,  2800] loss: 0.021\n",
      "[8,  3000] loss: 0.021\n",
      "[8,  3200] loss: 0.022\n",
      "[8,  3400] loss: 0.021\n",
      "[8,  3600] loss: 0.022\n",
      "[8,  3800] loss: 0.020\n",
      "[8,  4000] loss: 0.020\n",
      "[8,  4200] loss: 0.020\n",
      "[8,  4400] loss: 0.021\n",
      "[8,  4600] loss: 0.021\n",
      "[8,  4800] loss: 0.021\n",
      "[8,  5000] loss: 0.022\n",
      "[8,  5200] loss: 0.024\n",
      "[9,   200] loss: 0.021\n",
      "[9,   400] loss: 0.023\n",
      "[9,   600] loss: 0.021\n",
      "[9,   800] loss: 0.022\n",
      "[9,  1000] loss: 0.019\n",
      "[9,  1200] loss: 0.021\n",
      "[9,  1400] loss: 0.021\n",
      "[9,  1600] loss: 0.021\n",
      "[9,  1800] loss: 0.021\n",
      "[9,  2000] loss: 0.021\n",
      "[9,  2200] loss: 0.023\n",
      "[9,  2400] loss: 0.023\n",
      "[9,  2600] loss: 0.021\n",
      "[9,  2800] loss: 0.020\n",
      "[9,  3000] loss: 0.021\n",
      "[9,  3200] loss: 0.021\n",
      "[9,  3400] loss: 0.021\n",
      "[9,  3600] loss: 0.022\n",
      "[9,  3800] loss: 0.020\n",
      "[9,  4000] loss: 0.019\n",
      "[9,  4200] loss: 0.020\n",
      "[9,  4400] loss: 0.020\n",
      "[9,  4600] loss: 0.020\n",
      "[9,  4800] loss: 0.020\n",
      "[9,  5000] loss: 0.021\n",
      "[9,  5200] loss: 0.023\n",
      "[10,   200] loss: 0.020\n",
      "[10,   400] loss: 0.022\n",
      "[10,   600] loss: 0.020\n",
      "[10,   800] loss: 0.021\n",
      "[10,  1000] loss: 0.019\n",
      "[10,  1200] loss: 0.021\n",
      "[10,  1400] loss: 0.020\n",
      "[10,  1600] loss: 0.020\n",
      "[10,  1800] loss: 0.020\n",
      "[10,  2000] loss: 0.021\n",
      "[10,  2200] loss: 0.022\n",
      "[10,  2400] loss: 0.022\n",
      "[10,  2600] loss: 0.020\n",
      "[10,  2800] loss: 0.019\n",
      "[10,  3000] loss: 0.020\n",
      "[10,  3200] loss: 0.020\n",
      "[10,  3400] loss: 0.020\n",
      "[10,  3600] loss: 0.021\n",
      "[10,  3800] loss: 0.019\n",
      "[10,  4000] loss: 0.019\n",
      "[10,  4200] loss: 0.019\n",
      "[10,  4400] loss: 0.019\n",
      "[10,  4600] loss: 0.019\n",
      "[10,  4800] loss: 0.019\n",
      "[10,  5000] loss: 0.020\n",
      "[10,  5200] loss: 0.022\n"
     ]
    }
   ],
   "source": [
    "# train a new model on all train dataset\n",
    "\n",
    "net = Net(h_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "for epoch in range(max_iters):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(training_set):\n",
    "        inputs = torch.from_numpy(data)\n",
    "        labels = training_set_labels[i] \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.from_numpy(np.array(labels).astype(np.longlong)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print every 200 mini-batches\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a new neural network on the full train dataset\n",
    "\n",
    "full_dataset = xTrainingSet + xValidationSet\n",
    "full_labels = yTrainingSet + yValidationSet\n",
    "\n",
    "batch_size = 25\n",
    "training_set, training_set_labels = create_batches(full_dataset, full_labels, batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    snips, n_dig = find_digits(sample, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test data through find digits\n",
    "# run each image of net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-validation above for hyper-parameter tuning\n",
    "# then use found hyper-parameters to train model with entire dataset (not separated into train and validation)\n",
    "# use model on unseen data -> test set"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
