{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBGRImage(path):\n",
    "    image = BGR(cv.imread(path))\n",
    "    return image\n",
    "\n",
    "def loadGreyImage(path):\n",
    "    image = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
    "    return image\n",
    "\n",
    "def BGR(image):\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "def displayGreyImage(image, imageName):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "\n",
    "def displayGreyWindows(image, imageName):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def displayBGRImage(image, imageName, size):\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(image)\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def displayBGRImageLarge(image, imageName):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    plt.imshow(image)\n",
    "    plt.title(imageName)\n",
    "    plt.show()\n",
    "    \n",
    "def imageSideBySide(images, imageNames,size):\n",
    "    row = np.ceil(len(images)/20)\n",
    "    fig=plt.figure(figsize=(size, size/2))\n",
    "    for i, image in enumerate(images):\n",
    "        fig.add_subplot(row, 20, i+1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(imageNames[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_dataset', 'train_dataset', 'train_labels']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 25, 26\n",
    "size = 14, 12\n",
    "\n",
    "f = h5py.File('MNIST_synthetic.h5', 'r')\n",
    "\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.squeeze(np.array(f[\"train_dataset\"])).copy()\n",
    "train_labels = np.squeeze(np.array(f[\"train_labels\"])).copy()\n",
    "test_dataset = np.squeeze(np.array(f[\"test_dataset\"])).copy()\n",
    "\n",
    "singulars_digits = []\n",
    "singulars_labels = []\n",
    "\n",
    "doubles_digits = []\n",
    "doubles_labels = []\n",
    "\n",
    "triples_digits = []\n",
    "triples_labels = []\n",
    "\n",
    "quadruples_digits = []\n",
    "quadruples_labels = []\n",
    "\n",
    "quintuples_digits = []\n",
    "quintuples_labels = []\n",
    "\n",
    "\n",
    "for i, labels in enumerate(train_labels):\n",
    "    if labels[1] == 10:\n",
    "        singulars_digits.append(train_dataset[i])\n",
    "        singulars_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[1] != 10 and labels[2] == 10:\n",
    "        doubles_digits.append(train_dataset[i])\n",
    "        doubles_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[2] != 10 and labels[3] == 10:\n",
    "        triples_digits.append(train_dataset[i])\n",
    "        triples_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[3] != 10 and labels[4] == 10:\n",
    "        quadruples_digits.append(train_dataset[i])\n",
    "        quadruples_labels.append(train_labels[i])\n",
    "        \n",
    "    if labels[4] != 10:\n",
    "        quintuples_digits.append(train_dataset[i])\n",
    "        quintuples_labels.append(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "singulars_digits = np.array(singulars_digits)     \n",
    "doubles_digits = np.array(doubles_digits)  \n",
    "triples_digits = np.array(triples_digits)    \n",
    "quadruples_digits = np.array(quadruples_digits)    \n",
    "quintuples_digits = np.array(quintuples_digits)    \n",
    "\n",
    "singulars_labels = np.array(singulars_labels).T[0]\n",
    "doubles_labels = np.array(doubles_labels).T[0:2].T\n",
    "triples_label = np.array(triples_labels).T[0:3].T\n",
    "quadruples_label = np.array(quadruples_labels).T[0:4].T\n",
    "quintuples_label = np.array(quintuples_labels).T[0:5].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_singulars_dataset = []\n",
    "final_singulars_labels = list(singulars_labels)\n",
    "for i, digit in enumerate(singulars_digits):\n",
    "    final_singulars_dataset.append(digit[26:38,26:38])\n",
    "    \n",
    "final_doubles_dataset = []\n",
    "final_doubles_labels = []\n",
    "for i, digit in enumerate(doubles_digits):\n",
    "    final_doubles_dataset.append(digit[26:38,20:32])\n",
    "    final_doubles_dataset.append(digit[26:38,32:44])\n",
    "    final_doubles_labels.append(doubles_labels[i][0])\n",
    "    final_doubles_labels.append(doubles_labels[i][1])\n",
    "\n",
    "final_triples_dataset = []\n",
    "final_triples_labels = []\n",
    "for i, digit in enumerate(triples_digits):\n",
    "    final_triples_dataset.append(digit[26:38,14:26])\n",
    "    final_triples_dataset.append(digit[26:38,26:38])\n",
    "    final_triples_dataset.append(digit[26:38,38:50])\n",
    "    final_triples_labels.append(triples_labels[i][0])\n",
    "    final_triples_labels.append(triples_labels[i][1])\n",
    "    final_triples_labels.append(triples_labels[i][2])\n",
    "    \n",
    "final_quadruples_dataset = []\n",
    "final_quadruples_labels = []\n",
    "for i, digit in enumerate(quadruples_digits):\n",
    "    final_quadruples_dataset.append(digit[26:38,8:20])\n",
    "    final_quadruples_dataset.append(digit[26:38,20:32])\n",
    "    final_quadruples_dataset.append(digit[26:38,32:44])\n",
    "    final_quadruples_dataset.append(digit[26:38,44:56])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][0])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][1])    \n",
    "    final_quadruples_labels.append(quadruples_labels[i][2])\n",
    "    final_quadruples_labels.append(quadruples_labels[i][3]) \n",
    "    \n",
    "final_quintuples_dataset = []\n",
    "final_quintuples_labels = []\n",
    "for i, digit in enumerate(quintuples_digits):\n",
    "    final_quintuples_dataset.append(digit[26:38,2:14])\n",
    "    final_quintuples_dataset.append(digit[26:38,14:26])\n",
    "    final_quintuples_dataset.append(digit[26:38,26:38])\n",
    "    final_quintuples_dataset.append(digit[26:38,38:50])\n",
    "    final_quintuples_dataset.append(digit[26:38,50:62])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][0])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][1])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][2])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][3])\n",
    "    final_quintuples_labels.append(quintuples_labels[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = final_singulars_dataset + final_doubles_dataset + final_triples_dataset + final_quadruples_dataset + final_quintuples_dataset\n",
    "merged_labels = final_singulars_labels + final_doubles_labels + final_triples_labels + final_quadruples_labels + final_quintuples_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 0's in train dataset: 16379\n",
      "number of 1's in train dataset: 18819\n",
      "number of 2's in train dataset: 17095\n",
      "number of 3's in train dataset: 17220\n",
      "number of 4's in train dataset: 16393\n",
      "number of 5's in train dataset: 15275\n",
      "number of 6's in train dataset: 16401\n",
      "number of 7's in train dataset: 17611\n",
      "number of 8's in train dataset: 16175\n",
      "number of 9's in train dataset: 16518\n",
      "total number of digits in train dataset: 167886\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for digit in range(10):\n",
    "    digit_count = list(merged_labels).count(digit)\n",
    "    print(f\"number of {digit}'s in train dataset: {digit_count}\")\n",
    "    count += digit_count\n",
    "\n",
    "print(\"total number of digits in train dataset:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(merged_dataset)))\n",
    "digitsValidationSetSize = int(len(merged_labels) - digitsTrainingSetSize)\n",
    "\n",
    "xValidationSet = []\n",
    "yValidationSet = []\n",
    "\n",
    "for index, digit in enumerate(merged_dataset[0:digitsValidationSetSize]):\n",
    "    xValidationSet.append(digit)\n",
    "    yValidationSet.append(merged_labels[index])\n",
    "\n",
    "xTrainingSet = []\n",
    "yTrainingSet = []\n",
    "\n",
    "start = len(xValidationSet)\n",
    "\n",
    "for i, digit in enumerate(merged_dataset[start:]):\n",
    "    xTrainingSet.append(digit)\n",
    "    yTrainingSet.append(merged_labels[i+start])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding all digits in an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_digits(train_dataset, size):\n",
    "    thresh_train_dataset = train_dataset.copy()\n",
    "    thresh_train_dataset[thresh_train_dataset>30] = 255\n",
    "    thresh_train_dataset[thresh_train_dataset!=255] = 0\n",
    "    i,j = np.where(thresh_train_dataset[:,:]!=0)\n",
    "    #print(i)\n",
    "    bottom_bound = np.min(i)\n",
    "    top_bound = np.max(i)\n",
    "    left_bound = np.min(j)\n",
    "    right_bound = np.max(j)\n",
    "    #print(top_bound, bottom_bound)\n",
    "\n",
    "    h = top_bound-bottom_bound\n",
    "    w = right_bound-left_bound\n",
    "\n",
    "    bounded_train_dataset = train_dataset[bottom_bound-1:top_bound+2,left_bound-1:right_bound+2]\n",
    "    thresh = bounded_train_dataset.copy()\n",
    "    thresh[thresh>30] = 255\n",
    "    thresh[thresh!=255] = 0\n",
    "    #ret, thresh = cv2.threshold(bounded_train_dataset, 30, 255, 0)\n",
    "    #seg = np.where(np.any(thresh, axis=0)==0)\n",
    "    seg = np.where(np.any(thresh, axis=0)==0)\n",
    "\n",
    "    #print(seg)\n",
    "    seg_list = np.asarray(seg)\n",
    "    seg_list = seg_list[0]\n",
    "    #print(seg_list)\n",
    "    from statistics import stdev\n",
    "\n",
    "    if len(seg_list)>2:\n",
    "        # create a list of the gaps between the consecutive values\n",
    "        gaps = [y - x for x, y in zip(seg_list[:-1], seg_list[1:])]\n",
    "        # have python calculate the standard deviation for the gaps\n",
    "        sd = stdev(gaps)\n",
    "\n",
    "        # create a list of lists, put the first value of the source data in the first\n",
    "        lists = [[seg_list[0]]]\n",
    "        for x in seg_list[1:]:\n",
    "            # if the gap from the current item to the previous is more than 1 SD\n",
    "            # Note: the previous item is the last item in the last list\n",
    "            # Note: the '> 1' is the part you'd modify to make it stricter or more relaxed\n",
    "            if (x - lists[-1][-1]) / (sd+1e-18) > 0.8:\n",
    "                # then start a new list\n",
    "                lists.append([])\n",
    "            # add the current item to the last list in the list\n",
    "            lists[-1].append(x)\n",
    "\n",
    "        splits = np.asarray([np.ceil(np.mean(lists[i])) for i in range(len(lists))]).astype(int)\n",
    "\n",
    "    else:\n",
    "        splits = np.asarray(seg_list)\n",
    "    #print(splits)\n",
    "\n",
    "    n_digits = len(splits)-1\n",
    "    #digits = np.zeros(n_digits, )\n",
    "    digits = []\n",
    "    for i in range(n_digits):\n",
    "        temp = bounded_train_dataset[:,splits[i]:splits[i+1]]\n",
    "        # if temp is less than recommeded size first pad on left and then on both sides\n",
    "        temp_padded = temp.copy()\n",
    "        if (temp.shape[0] != size) or  (temp.shape[1] != size):\n",
    "            diff_y = size-temp.shape[0]\n",
    "            split_diff_y = diff_y//2\n",
    "            remainder_diff_y = diff_y%2\n",
    "            diff_x = size-temp.shape[1]\n",
    "            split_diff_x = diff_x//2\n",
    "            remainder_diff_x = diff_x%2\n",
    "#             print(diff_y)\n",
    "#             print(split_diff_y)\n",
    "            temp_padded = np.pad(temp, ((split_diff_y+remainder_diff_y,split_diff_y),(split_diff_x+remainder_diff_x,split_diff_x)))\n",
    "\n",
    "        digits.append(temp_padded)\n",
    "    return digits, n_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect at index: 367\n",
      "incorrect at index: 1351\n",
      "incorrect at index: 1606\n",
      "incorrect at index: 2684\n",
      "incorrect at index: 3172\n",
      "incorrect at index: 3771\n",
      "incorrect at index: 3838\n",
      "incorrect at index: 3883\n",
      "incorrect at index: 4011\n",
      "incorrect at index: 4447\n",
      "incorrect at index: 4639\n",
      "incorrect at index: 4747\n",
      "incorrect at index: 5419\n",
      "incorrect at index: 5442\n",
      "incorrect at index: 5772\n",
      "incorrect at index: 5930\n",
      "incorrect at index: 7118\n",
      "incorrect at index: 7981\n",
      "incorrect at index: 8652\n",
      "incorrect at index: 8860\n",
      "incorrect at index: 8941\n",
      "incorrect at index: 9038\n",
      "incorrect at index: 9176\n",
      "incorrect at index: 9419\n",
      "incorrect at index: 10702\n",
      "incorrect at index: 11622\n",
      "incorrect at index: 11810\n",
      "incorrect at index: 12806\n",
      "incorrect at index: 13280\n",
      "incorrect at index: 13360\n",
      "incorrect at index: 13514\n",
      "incorrect at index: 14150\n",
      "incorrect at index: 14931\n",
      "incorrect at index: 15844\n",
      "incorrect at index: 16366\n",
      "incorrect at index: 16909\n",
      "incorrect at index: 17030\n",
      "incorrect at index: 17540\n",
      "incorrect at index: 17573\n",
      "incorrect at index: 18122\n",
      "incorrect at index: 18127\n",
      "incorrect at index: 18460\n",
      "incorrect at index: 19104\n",
      "incorrect at index: 19660\n",
      "incorrect at index: 20029\n",
      "incorrect at index: 20764\n",
      "incorrect at index: 21153\n",
      "incorrect at index: 21854\n",
      "incorrect at index: 21980\n",
      "incorrect at index: 22025\n",
      "incorrect at index: 22369\n",
      "incorrect at index: 22596\n",
      "incorrect at index: 23044\n",
      "incorrect at index: 23684\n",
      "incorrect at index: 24026\n",
      "incorrect at index: 24518\n",
      "incorrect at index: 25190\n",
      "incorrect at index: 25233\n",
      "incorrect at index: 25273\n",
      "incorrect at index: 26042\n",
      "incorrect at index: 26094\n",
      "incorrect at index: 26601\n",
      "incorrect at index: 27704\n",
      "incorrect at index: 28516\n",
      "incorrect at index: 28518\n",
      "incorrect at index: 28802\n",
      "incorrect at index: 29359\n",
      "incorrect at index: 29444\n",
      "incorrect at index: 29616\n",
      "incorrect at index: 29775\n",
      "incorrect at index: 30169\n",
      "incorrect at index: 31388\n",
      "incorrect at index: 32154\n",
      "incorrect at index: 32609\n",
      "incorrect at index: 33049\n",
      "incorrect at index: 33071\n",
      "incorrect at index: 33204\n",
      "incorrect at index: 33908\n",
      "incorrect at index: 34014\n",
      "incorrect at index: 34814\n",
      "incorrect at index: 34852\n",
      "incorrect at index: 34877\n",
      "incorrect at index: 35542\n",
      "incorrect at index: 36288\n",
      "incorrect at index: 36777\n",
      "incorrect at index: 37607\n",
      "incorrect at index: 37788\n",
      "incorrect at index: 37869\n",
      "incorrect at index: 37986\n",
      "incorrect at index: 39113\n",
      "incorrect at index: 39226\n",
      "incorrect at index: 39274\n",
      "incorrect at index: 39648\n",
      "incorrect at index: 39758\n",
      "incorrect at index: 39896\n",
      "incorrect at index: 40225\n",
      "incorrect at index: 40457\n",
      "incorrect at index: 41329\n",
      "incorrect at index: 41954\n",
      "incorrect at index: 42009\n",
      "incorrect at index: 42555\n",
      "incorrect at index: 42678\n",
      "incorrect at index: 44367\n",
      "incorrect at index: 44518\n",
      "incorrect at index: 44650\n",
      "incorrect at index: 45004\n",
      "incorrect at index: 45382\n",
      "incorrect at index: 46543\n",
      "incorrect at index: 46663\n",
      "incorrect at index: 46759\n",
      "incorrect at index: 46979\n",
      "incorrect at index: 47846\n",
      "incorrect at index: 47884\n",
      "incorrect at index: 48964\n",
      "incorrect at index: 49315\n",
      "incorrect at index: 49406\n",
      "incorrect at index: 50272\n",
      "incorrect at index: 50383\n",
      "incorrect at index: 50715\n",
      "incorrect at index: 50729\n",
      "incorrect at index: 51103\n",
      "incorrect at index: 51106\n",
      "incorrect at index: 52290\n",
      "incorrect at index: 52762\n",
      "incorrect at index: 53061\n",
      "incorrect at index: 54031\n",
      "incorrect at index: 54206\n",
      "incorrect at index: 54314\n",
      "incorrect at index: 54356\n",
      "incorrect at index: 54528\n",
      "incorrect at index: 55379\n",
      "incorrect at index: 55470\n",
      "incorrect at index: 55693\n",
      "incorrect at index: 55849\n",
      "incorrect at index: 55866\n",
      "total percentage incorrect: 0.24107142857142855 %\n"
     ]
    }
   ],
   "source": [
    "wrong = 0 \n",
    "wrong_arr = []\n",
    "wrong_arr_index = []\n",
    "correct = []\n",
    "\n",
    "for i, sample in enumerate(train_dataset):\n",
    "    snips, n_dig = find_digits(sample, 25)\n",
    "    real_num_digits = 5 - list(train_labels[i]).count(10)\n",
    "\n",
    "    if(n_dig != real_num_digits):\n",
    "        print(\"incorrect at index:\", i)\n",
    "        wrong += 1\n",
    "        wrong_arr.append(snips)\n",
    "        wrong_arr_index.append(i)  \n",
    "\n",
    "print(\"total percentage incorrect:\", wrong / len(train_dataset) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "num_test_digits = 0\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    images, n_digits = find_digits(sample, 25)\n",
    "   \n",
    "    num_test_digits += n_digits\n",
    "\n",
    "divs = []\n",
    "\n",
    "for div in range(1, 100):\n",
    "    if(num_test_digits % (div) == 0):\n",
    "        divs.append(div)\n",
    "\n",
    "batch_size = divs[-1]\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(input_array, label_array, batch_size):\n",
    "    batched = []\n",
    "    label_batched = []\n",
    "    \n",
    "    for i in range(np.floor(len(input_array)/batch_size).astype(int)):\n",
    "        batched.append(np.expand_dims((np.array(input_array[i*batch_size:i*batch_size+batch_size])).astype(np.single),axis=1))\n",
    "        label_batched.append(label_array[i*batch_size:i*batch_size+batch_size])\n",
    "        \n",
    "    return np.array(batched), label_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, h_layers):\n",
    "        self.num_layers = len(h_layers)\n",
    "        self.h_layers = h_layers      \n",
    "        self.convs = []\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        for i, n in enumerate(self.h_layers):\n",
    "            if i == 0:\n",
    "                self.convs.append(nn.Conv2d(1, n, 3))\n",
    "            else:\n",
    "                self.convs.append(nn.Conv2d(self.h_layers[i-1], n, 3))\n",
    "\n",
    "        self.fc1 = nn.Linear(self.h_layers[-1]**2 , 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.h_layers)):\n",
    "            x = F.relu(self.convs[i](x))\n",
    "            \n",
    "        x = x.view(-1, self.h_layers[-1]**2)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN for hyper-parameter testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 2.140\n",
      "[1,    40] loss: 1.644\n",
      "[1,    60] loss: 1.280\n",
      "[1,    80] loss: 1.069\n",
      "[1,   100] loss: 0.953\n",
      "[1,   120] loss: 0.830\n",
      "[1,   140] loss: 0.798\n",
      "[1,   160] loss: 0.725\n",
      "[1,   180] loss: 0.671\n",
      "[1,   200] loss: 0.642\n",
      "[1,   220] loss: 0.648\n",
      "[1,   240] loss: 0.604\n",
      "[1,   260] loss: 0.597\n",
      "[1,   280] loss: 0.558\n",
      "[1,   300] loss: 0.509\n",
      "[1,   320] loss: 0.520\n",
      "[1,   340] loss: 0.531\n",
      "[1,   360] loss: 0.510\n",
      "[1,   380] loss: 0.501\n",
      "[1,   400] loss: 0.500\n",
      "[1,   420] loss: 0.493\n",
      "[1,   440] loss: 0.491\n",
      "[1,   460] loss: 0.457\n",
      "[1,   480] loss: 0.455\n",
      "[1,   500] loss: 0.454\n",
      "[1,   520] loss: 0.435\n",
      "[1,   540] loss: 0.427\n",
      "[1,   560] loss: 0.457\n",
      "[1,   580] loss: 0.457\n",
      "[1,   600] loss: 0.429\n",
      "[1,   620] loss: 0.421\n",
      "[1,   640] loss: 0.415\n",
      "[1,   660] loss: 0.427\n",
      "[1,   680] loss: 0.436\n",
      "[1,   700] loss: 0.428\n",
      "[1,   720] loss: 0.418\n",
      "[1,   740] loss: 0.453\n",
      "[1,   760] loss: 0.403\n",
      "[1,   780] loss: 0.397\n",
      "[1,   800] loss: 0.390\n",
      "[1,   820] loss: 0.381\n",
      "[1,   840] loss: 0.377\n",
      "[1,   860] loss: 0.362\n",
      "[1,   880] loss: 0.358\n",
      "[1,   900] loss: 0.348\n",
      "[1,   920] loss: 0.398\n",
      "[1,   940] loss: 0.373\n",
      "[1,   960] loss: 0.381\n",
      "[1,   980] loss: 0.383\n",
      "[1,  1000] loss: 0.367\n",
      "[1,  1020] loss: 0.371\n",
      "[1,  1040] loss: 0.359\n",
      "[1,  1060] loss: 0.347\n",
      "[1,  1080] loss: 0.353\n",
      "[1,  1100] loss: 0.372\n",
      "[1,  1120] loss: 0.345\n",
      "[1,  1140] loss: 0.369\n",
      "[1,  1160] loss: 0.354\n",
      "[1,  1180] loss: 0.354\n",
      "[1,  1200] loss: 0.301\n",
      "[1,  1220] loss: 0.338\n",
      "[1,  1240] loss: 0.304\n",
      "[1,  1260] loss: 0.342\n",
      "[1,  1280] loss: 0.339\n",
      "[1,  1300] loss: 0.324\n",
      "[1,  1320] loss: 0.321\n",
      "[1,  1340] loss: 0.301\n",
      "[1,  1360] loss: 0.341\n",
      "[1,  1380] loss: 0.325\n",
      "[1,  1400] loss: 0.306\n",
      "[1,  1420] loss: 0.344\n",
      "[1,  1440] loss: 0.332\n",
      "[1,  1460] loss: 0.290\n",
      "[1,  1480] loss: 0.307\n",
      "[1,  1500] loss: 0.329\n",
      "[1,  1520] loss: 0.310\n",
      "[1,  1540] loss: 0.328\n",
      "[1,  1560] loss: 0.325\n",
      "[1,  1580] loss: 0.325\n",
      "[1,  1600] loss: 0.330\n",
      "[1,  1620] loss: 0.367\n",
      "[1,  1640] loss: 0.327\n",
      "[1,  1660] loss: 0.320\n",
      "[1,  1680] loss: 0.319\n",
      "[1,  1700] loss: 0.316\n",
      "[2,    20] loss: 0.312\n",
      "[2,    40] loss: 0.323\n",
      "[2,    60] loss: 0.279\n",
      "[2,    80] loss: 0.342\n",
      "[2,   100] loss: 0.322\n",
      "[2,   120] loss: 0.289\n",
      "[2,   140] loss: 0.337\n",
      "[2,   160] loss: 0.297\n",
      "[2,   180] loss: 0.292\n",
      "[2,   200] loss: 0.286\n",
      "[2,   220] loss: 0.326\n",
      "[2,   240] loss: 0.294\n",
      "[2,   260] loss: 0.309\n",
      "[2,   280] loss: 0.285\n",
      "[2,   300] loss: 0.256\n",
      "[2,   320] loss: 0.280\n",
      "[2,   340] loss: 0.307\n",
      "[2,   360] loss: 0.290\n",
      "[2,   380] loss: 0.286\n",
      "[2,   400] loss: 0.300\n",
      "[2,   420] loss: 0.287\n",
      "[2,   440] loss: 0.293\n",
      "[2,   460] loss: 0.280\n",
      "[2,   480] loss: 0.299\n",
      "[2,   500] loss: 0.284\n",
      "[2,   520] loss: 0.269\n",
      "[2,   540] loss: 0.282\n",
      "[2,   560] loss: 0.296\n",
      "[2,   580] loss: 0.302\n",
      "[2,   600] loss: 0.286\n",
      "[2,   620] loss: 0.284\n",
      "[2,   640] loss: 0.276\n",
      "[2,   660] loss: 0.304\n",
      "[2,   680] loss: 0.310\n",
      "[2,   700] loss: 0.296\n",
      "[2,   720] loss: 0.293\n",
      "[2,   740] loss: 0.331\n",
      "[2,   760] loss: 0.282\n",
      "[2,   780] loss: 0.281\n",
      "[2,   800] loss: 0.278\n",
      "[2,   820] loss: 0.270\n",
      "[2,   840] loss: 0.272\n",
      "[2,   860] loss: 0.257\n",
      "[2,   880] loss: 0.250\n",
      "[2,   900] loss: 0.251\n",
      "[2,   920] loss: 0.294\n",
      "[2,   940] loss: 0.266\n",
      "[2,   960] loss: 0.282\n",
      "[2,   980] loss: 0.286\n",
      "[2,  1000] loss: 0.266\n",
      "[2,  1020] loss: 0.273\n",
      "[2,  1040] loss: 0.268\n",
      "[2,  1060] loss: 0.255\n",
      "[2,  1080] loss: 0.262\n",
      "[2,  1100] loss: 0.287\n",
      "[2,  1120] loss: 0.267\n",
      "[2,  1140] loss: 0.284\n",
      "[2,  1160] loss: 0.265\n",
      "[2,  1180] loss: 0.268\n",
      "[2,  1200] loss: 0.224\n",
      "[2,  1220] loss: 0.262\n",
      "[2,  1240] loss: 0.226\n",
      "[2,  1260] loss: 0.264\n",
      "[2,  1280] loss: 0.263\n",
      "[2,  1300] loss: 0.254\n",
      "[2,  1320] loss: 0.247\n",
      "[2,  1340] loss: 0.228\n",
      "[2,  1360] loss: 0.270\n",
      "[2,  1380] loss: 0.249\n",
      "[2,  1400] loss: 0.238\n",
      "[2,  1420] loss: 0.273\n",
      "[2,  1440] loss: 0.262\n",
      "[2,  1460] loss: 0.225\n",
      "[2,  1480] loss: 0.243\n",
      "[2,  1500] loss: 0.263\n",
      "[2,  1520] loss: 0.243\n",
      "[2,  1540] loss: 0.264\n",
      "[2,  1560] loss: 0.257\n",
      "[2,  1580] loss: 0.260\n",
      "[2,  1600] loss: 0.267\n",
      "[2,  1620] loss: 0.305\n",
      "[2,  1640] loss: 0.265\n",
      "[2,  1660] loss: 0.258\n",
      "[2,  1680] loss: 0.259\n",
      "[2,  1700] loss: 0.255\n",
      "[3,    20] loss: 0.253\n",
      "[3,    40] loss: 0.262\n",
      "[3,    60] loss: 0.218\n",
      "[3,    80] loss: 0.285\n",
      "[3,   100] loss: 0.262\n",
      "[3,   120] loss: 0.237\n",
      "[3,   140] loss: 0.280\n",
      "[3,   160] loss: 0.245\n",
      "[3,   180] loss: 0.241\n",
      "[3,   200] loss: 0.237\n",
      "[3,   220] loss: 0.273\n",
      "[3,   240] loss: 0.242\n",
      "[3,   260] loss: 0.257\n",
      "[3,   280] loss: 0.236\n",
      "[3,   300] loss: 0.209\n",
      "[3,   320] loss: 0.231\n",
      "[3,   340] loss: 0.257\n",
      "[3,   360] loss: 0.241\n",
      "[3,   380] loss: 0.236\n",
      "[3,   400] loss: 0.250\n",
      "[3,   420] loss: 0.236\n",
      "[3,   440] loss: 0.242\n",
      "[3,   460] loss: 0.237\n",
      "[3,   480] loss: 0.258\n",
      "[3,   500] loss: 0.237\n",
      "[3,   520] loss: 0.225\n",
      "[3,   540] loss: 0.241\n",
      "[3,   560] loss: 0.248\n",
      "[3,   580] loss: 0.255\n",
      "[3,   600] loss: 0.243\n",
      "[3,   620] loss: 0.241\n",
      "[3,   640] loss: 0.231\n",
      "[3,   660] loss: 0.263\n",
      "[3,   680] loss: 0.266\n",
      "[3,   700] loss: 0.251\n",
      "[3,   720] loss: 0.251\n",
      "[3,   740] loss: 0.285\n",
      "[3,   760] loss: 0.239\n",
      "[3,   780] loss: 0.238\n",
      "[3,   800] loss: 0.238\n",
      "[3,   820] loss: 0.230\n",
      "[3,   840] loss: 0.233\n",
      "[3,   860] loss: 0.220\n",
      "[3,   880] loss: 0.211\n",
      "[3,   900] loss: 0.213\n",
      "[3,   920] loss: 0.253\n",
      "[3,   940] loss: 0.225\n",
      "[3,   960] loss: 0.243\n",
      "[3,   980] loss: 0.247\n",
      "[3,  1000] loss: 0.225\n",
      "[3,  1020] loss: 0.234\n",
      "[3,  1040] loss: 0.231\n",
      "[3,  1060] loss: 0.218\n",
      "[3,  1080] loss: 0.223\n",
      "[3,  1100] loss: 0.251\n",
      "[3,  1120] loss: 0.233\n",
      "[3,  1140] loss: 0.248\n",
      "[3,  1160] loss: 0.226\n",
      "[3,  1180] loss: 0.231\n",
      "[3,  1200] loss: 0.192\n",
      "[3,  1220] loss: 0.228\n",
      "[3,  1240] loss: 0.192\n",
      "[3,  1260] loss: 0.228\n",
      "[3,  1280] loss: 0.227\n",
      "[3,  1300] loss: 0.222\n",
      "[3,  1320] loss: 0.215\n",
      "[3,  1340] loss: 0.196\n",
      "[3,  1360] loss: 0.237\n",
      "[3,  1380] loss: 0.213\n",
      "[3,  1400] loss: 0.208\n",
      "[3,  1420] loss: 0.238\n",
      "[3,  1440] loss: 0.227\n",
      "[3,  1460] loss: 0.194\n",
      "[3,  1480] loss: 0.212\n",
      "[3,  1500] loss: 0.230\n",
      "[3,  1520] loss: 0.210\n",
      "[3,  1540] loss: 0.230\n",
      "[3,  1560] loss: 0.224\n",
      "[3,  1580] loss: 0.227\n",
      "[3,  1600] loss: 0.235\n",
      "[3,  1620] loss: 0.272\n",
      "[3,  1640] loss: 0.232\n",
      "[3,  1660] loss: 0.227\n",
      "[3,  1680] loss: 0.227\n",
      "[3,  1700] loss: 0.224\n",
      "[4,    20] loss: 0.221\n",
      "[4,    40] loss: 0.229\n",
      "[4,    60] loss: 0.187\n",
      "[4,    80] loss: 0.254\n",
      "[4,   100] loss: 0.230\n",
      "[4,   120] loss: 0.209\n",
      "[4,   140] loss: 0.247\n",
      "[4,   160] loss: 0.217\n",
      "[4,   180] loss: 0.212\n",
      "[4,   200] loss: 0.210\n",
      "[4,   220] loss: 0.244\n",
      "[4,   240] loss: 0.214\n",
      "[4,   260] loss: 0.229\n",
      "[4,   280] loss: 0.210\n",
      "[4,   300] loss: 0.183\n",
      "[4,   320] loss: 0.204\n",
      "[4,   340] loss: 0.228\n",
      "[4,   360] loss: 0.214\n",
      "[4,   380] loss: 0.208\n",
      "[4,   400] loss: 0.223\n",
      "[4,   420] loss: 0.208\n",
      "[4,   440] loss: 0.214\n",
      "[4,   460] loss: 0.212\n",
      "[4,   480] loss: 0.234\n",
      "[4,   500] loss: 0.210\n",
      "[4,   520] loss: 0.200\n",
      "[4,   540] loss: 0.217\n",
      "[4,   560] loss: 0.220\n",
      "[4,   580] loss: 0.227\n",
      "[4,   600] loss: 0.218\n",
      "[4,   620] loss: 0.215\n",
      "[4,   640] loss: 0.205\n",
      "[4,   660] loss: 0.238\n",
      "[4,   680] loss: 0.239\n",
      "[4,   700] loss: 0.224\n",
      "[4,   720] loss: 0.226\n",
      "[4,   740] loss: 0.256\n",
      "[4,   760] loss: 0.212\n",
      "[4,   780] loss: 0.212\n",
      "[4,   800] loss: 0.214\n",
      "[4,   820] loss: 0.205\n",
      "[4,   840] loss: 0.209\n",
      "[4,   860] loss: 0.198\n",
      "[4,   880] loss: 0.187\n",
      "[4,   900] loss: 0.190\n",
      "[4,   920] loss: 0.228\n",
      "[4,   940] loss: 0.200\n",
      "[4,   960] loss: 0.219\n",
      "[4,   980] loss: 0.222\n",
      "[4,  1000] loss: 0.200\n",
      "[4,  1020] loss: 0.210\n",
      "[4,  1040] loss: 0.208\n",
      "[4,  1060] loss: 0.194\n",
      "[4,  1080] loss: 0.199\n",
      "[4,  1100] loss: 0.228\n",
      "[4,  1120] loss: 0.212\n",
      "[4,  1140] loss: 0.225\n",
      "[4,  1160] loss: 0.201\n",
      "[4,  1180] loss: 0.208\n",
      "[4,  1200] loss: 0.172\n",
      "[4,  1220] loss: 0.206\n",
      "[4,  1240] loss: 0.171\n",
      "[4,  1260] loss: 0.206\n",
      "[4,  1280] loss: 0.204\n",
      "[4,  1300] loss: 0.201\n",
      "[4,  1320] loss: 0.194\n",
      "[4,  1340] loss: 0.175\n",
      "[4,  1360] loss: 0.216\n",
      "[4,  1380] loss: 0.190\n",
      "[4,  1400] loss: 0.189\n",
      "[4,  1420] loss: 0.215\n",
      "[4,  1440] loss: 0.204\n",
      "[4,  1460] loss: 0.175\n",
      "[4,  1480] loss: 0.192\n",
      "[4,  1500] loss: 0.209\n",
      "[4,  1520] loss: 0.189\n",
      "[4,  1540] loss: 0.208\n",
      "[4,  1560] loss: 0.202\n",
      "[4,  1580] loss: 0.204\n",
      "[4,  1600] loss: 0.214\n",
      "[4,  1620] loss: 0.250\n",
      "[4,  1640] loss: 0.211\n",
      "[4,  1660] loss: 0.207\n",
      "[4,  1680] loss: 0.205\n",
      "[4,  1700] loss: 0.203\n",
      "[5,    20] loss: 0.200\n",
      "[5,    40] loss: 0.207\n",
      "[5,    60] loss: 0.168\n",
      "[5,    80] loss: 0.232\n",
      "[5,   100] loss: 0.209\n",
      "[5,   120] loss: 0.191\n",
      "[5,   140] loss: 0.224\n",
      "[5,   160] loss: 0.198\n",
      "[5,   180] loss: 0.193\n",
      "[5,   200] loss: 0.192\n",
      "[5,   220] loss: 0.223\n",
      "[5,   240] loss: 0.194\n",
      "[5,   260] loss: 0.209\n",
      "[5,   280] loss: 0.192\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9f79f16ae8b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_labels_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlonglong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-6a718bb0817b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    413\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 415\u001b[1;33m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[0;32m    416\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_dataset_batch, training_labels_batch = create_batches(xTrainingSet, yTrainingSet, batch_size)\n",
    "validation_dataset_batch, validation_labels_batch = create_batches(xValidationSet, yValidationSet, batch_size)\n",
    "\n",
    "# model hyper-parameters\n",
    "h_layers = ([batch_size, 64])\n",
    "max_iters = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "# train neural network\n",
    "net = Net(h_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "for epoch in range(max_iters):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(training_dataset_batch):\n",
    "        inputs = torch.from_numpy(data)\n",
    "        labels = training_labels_batch[i] \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.from_numpy(np.array(labels).astype(np.longlong)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 20 == 19:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0\n",
    "\n",
    "# test validation set on model\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(validation_dataset_batch):\n",
    "        images = torch.from_numpy(data)\n",
    "        labels = validation_labels_batch[i]\n",
    "        labels = torch.from_numpy(np.array(labels).astype(np.longlong))\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if (int(predicted[0]) != int(labels[0])):\n",
    "            None\n",
    "\n",
    "print(f\"validation set accuracy ({total} samples): {(100 * correct / total)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN to predict unseen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = xTrainingSet + xValidationSet\n",
    "full_labels = yTrainingSet + yValidationSet\n",
    "\n",
    "full_dataset_batch, full_labels_batch = create_batches(full_dataset, full_labels, batch_size)\n",
    "\n",
    "net = Net(h_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "for epoch in range(max_iters):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(full_dataset_batch):\n",
    "        inputs = torch.from_numpy(data)\n",
    "        labels = full_labels_batch[i] \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.from_numpy(np.array(labels).astype(np.longlong)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 20 == 19:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict unseen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_digits = []\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    digits = []\n",
    "    \n",
    "    images, n_digits = find_digits(sample, 25)\n",
    "   \n",
    "    for i, image in enumerate(images):\n",
    "        modified_image = image[7:19,7:19]\n",
    "        digits.append(modified_image)\n",
    "        \n",
    "    separated_digits.append(digits)\n",
    "    \n",
    "results = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    results.append([])\n",
    "\n",
    "row = 0\n",
    "col = 0\n",
    "    \n",
    "for i in range(int(num_test_digits / batch_size)):\n",
    "    batch = []\n",
    "    source = []\n",
    "    count = 0\n",
    "    \n",
    "    while count < batch_size:\n",
    "        batch.append(separated_digits[row][col])\n",
    "        source.append((row, col))\n",
    "        \n",
    "        if(col < len(separated_digits[row]) - 1):\n",
    "            col += 1\n",
    "        else:\n",
    "            row += 1\n",
    "            col = 0\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    batch = np.expand_dims(np.asarray(batch).astype(np.single), axis=1) \n",
    "    batch = torch.from_numpy(batch)\n",
    "    \n",
    "    output = net(batch)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    \n",
    "    for j, pred in enumerate(predicted):\n",
    "        coord = source[j]\n",
    "        results[coord[0]].append(pred.item())\n",
    "        \n",
    "for r in results:\n",
    "    pad = 5 - len(r)\n",
    "    \n",
    "    for i in range(pad):\n",
    "        r.append(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(test_dataset):\n",
    "    displayGreyWindows(sample, \"\")\n",
    "    print(results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
